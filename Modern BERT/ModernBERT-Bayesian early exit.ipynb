{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e26c312-8a54-4dbc-bd5d-2d935f8d9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449b3275-a509-45d5-a415-3faa06510a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, ModernBertForSequenceClassification\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "557778be-793b-4a31-9e5f-5d41c878e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import ModernBertForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "class ModernBERTBayesianEarlyExit:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"answerdotai/ModernBERT-base\",\n",
    "        exit_layers=[5, 10, 15, 20],\n",
    "        dropout_passes=10,          # Number of MC samples\n",
    "        uncertainty_threshold=0.02, # Lower = more confident\n",
    "        use_entropy=False           # use variance (default) or entropy\n",
    "    ):\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.model = ModernBertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # BUT dropout will still work because we unfreeze it manually later\n",
    "\n",
    "        # Extract structure\n",
    "        self.embeddings = self.model.model.embeddings\n",
    "        self.layers = self.model.model.layers\n",
    "        self.final_norm = self.model.model.final_norm\n",
    "        self.classifier = self.model.classifier\n",
    "\n",
    "        self.exit_layers = sorted(exit_layers)\n",
    "        self.dropout_passes = dropout_passes\n",
    "        self.uncertainty_threshold = uncertainty_threshold\n",
    "        self.use_entropy = use_entropy\n",
    "        self.num_layers = len(self.layers)\n",
    "\n",
    "        # Enable dropout modules even in eval mode\n",
    "        self._enable_dropout(self.model)\n",
    "\n",
    "    def _enable_dropout(self, module):\n",
    "        \"\"\"Force dropout modules to stay active even in eval mode (Bayesian trick).\"\"\"\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, torch.nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "    # ---------- Bayesian uncertainty metrics ----------\n",
    "    def predictive_variance(self, probs_mc):\n",
    "        \"\"\"\n",
    "        probs_mc: shape [K, num_classes]\n",
    "        Returns scalar uncertainty = variance of predicted probability for predicted class\n",
    "        \"\"\"\n",
    "        mean_prob = probs_mc.mean(dim=0)             # [num_classes]\n",
    "        var = ((probs_mc - mean_prob)**2).mean(dim=0)\n",
    "        pred_class = torch.argmax(mean_prob).item()\n",
    "        return var[pred_class].item(), mean_prob, pred_class\n",
    "\n",
    "    def predictive_entropy(self, probs):\n",
    "        \"\"\"\n",
    "        probs: mean probability vector\n",
    "        Entropy = -sum(p log p)\n",
    "        \"\"\"\n",
    "        entropy = -(probs * torch.log(probs + 1e-12)).sum().item()\n",
    "        return entropy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, text):\n",
    "        enc = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=False)\n",
    "        input_ids = enc[\"input_ids\"].to(self.device)\n",
    "\n",
    "        attention_mask = (enc[\"attention_mask\"] == 1).to(self.device)\n",
    "        position_ids = torch.arange(\n",
    "            0, input_ids.size(1), dtype=torch.long, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        hidden = self.embeddings(input_ids)\n",
    "\n",
    "        # Manual forward block by block\n",
    "        for i, layer in enumerate(self.layers):\n",
    "\n",
    "            hidden = layer(\n",
    "                hidden_states=hidden,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "            )[0]\n",
    "\n",
    "            # ---- BAYESIAN EXIT LAYER ----\n",
    "            if i in self.exit_layers:\n",
    "\n",
    "                cls = hidden[:, 0, :]  # CLS embedding\n",
    "\n",
    "                # MONTE CARLO DROPOUT\n",
    "                probs_mc = []\n",
    "                for _ in range(self.dropout_passes):\n",
    "                    logits = self.classifier(cls)\n",
    "                    probs_mc.append(F.softmax(logits, dim=-1))\n",
    "\n",
    "                probs_mc = torch.stack(probs_mc, dim=0).squeeze(1)   # FIX: remove batch dim\n",
    "\n",
    "                # Bayesian uncertainty (variance)\n",
    "                var, mean_prob, pred_class = self.predictive_variance(probs_mc)\n",
    "\n",
    "                # Alternatively use entropy\n",
    "                if self.use_entropy:\n",
    "                    entropy = self.predictive_entropy(mean_prob)\n",
    "                    unc_value = entropy\n",
    "                else:\n",
    "                    unc_value = var\n",
    "\n",
    "                # EXIT RULE\n",
    "                if unc_value <= self.uncertainty_threshold:\n",
    "                    conf = mean_prob[pred_class].item()\n",
    "                    return pred_class, i + 1, conf, unc_value\n",
    "\n",
    "        # Final layer (if no early exit)\n",
    "        hidden = self.final_norm(hidden)\n",
    "        cls = hidden[:, 0, :]\n",
    "        logits = self.classifier(cls)\n",
    "        final_probs = F.softmax(logits, dim=-1)[0]\n",
    "        conf, final_pred = torch.max(final_probs, dim=0)\n",
    "\n",
    "        return final_pred.item(), self.num_layers, conf.item(), 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f208a237-3bf9-4644-9323-1e578692f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBERTBaselineClassifier:\n",
    "    def __init__(self, model_name=\"answerdotai/ModernBERT-base\"):\n",
    "        # Detect device: MPS (Apple Silicon) → else CPU\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        # Load tokenizer + classification model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = ModernBertForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # ModernBERT layer count (22 for base)\n",
    "        self.num_layers = self.model.config.num_hidden_layers\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, text):\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits  # [1, num_labels]\n",
    "\n",
    "        # Softmax for confidence\n",
    "        probs = F.softmax(logits, dim=-1)[0]\n",
    "        conf, pred = torch.max(probs, dim=0)\n",
    "\n",
    "        return pred.item(), (self.num_layers - 1), conf.item(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f404c1-1a48-4294-bd68-13c41b288518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets once...\n",
      "\n",
      "Loading sst2...\n",
      "Loading imdb...\n",
      "Loading amazon_polarity...\n",
      "\n",
      "All datasets loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from evaluation.dataset_loaders.sst2 import load_sst2\n",
    "from evaluation.dataset_loaders.agnews import load_agnews\n",
    "from evaluation.dataset_loaders.amazon import load_amazon_polarity\n",
    "from evaluation.dataset_loaders.imdb import load_imdb\n",
    "from evaluation.dataset_loaders.dbpedia import load_dbpedia\n",
    "from evaluation.dataset_loaders.yanswers import load_yahoo\n",
    "\n",
    "dataset_loaders = [\n",
    "    (\"sst2\", load_sst2, \"classification\"),\n",
    "    (\"imdb\", load_imdb, \"classification\"),\n",
    "    (\"amazon_polarity\", load_amazon_polarity, \"classification\")\n",
    "]\n",
    "\n",
    "cached_datasets = {}\n",
    "print(\"Loading datasets once...\\n\")\n",
    "\n",
    "for name, loader, task in dataset_loaders:\n",
    "    print(f\"Loading {name}...\")\n",
    "    cached_datasets[name] = {\n",
    "        \"data\": loader(number=500),\n",
    "        \"task\": task\n",
    "    }\n",
    "\n",
    "print(\"\\nAll datasets loaded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c88862-88be-4b05-a563-6dab52f81bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract (text, label) from any format\n",
    "def extract_text_label(sample):\n",
    "    if isinstance(sample, dict):\n",
    "        if \"text\" in sample:\n",
    "            return sample[\"text\"], sample[\"label\"]\n",
    "        elif \"sentence\" in sample:\n",
    "            return sample[\"sentence\"], sample[\"label\"]\n",
    "        elif \"input_text\" in sample:\n",
    "            return sample[\"input_text\"], sample[\"label\"]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown dict format:\", sample)\n",
    "\n",
    "    if isinstance(sample, (tuple, list)):\n",
    "        return sample[0], sample[1]\n",
    "\n",
    "    raise ValueError(\"Unknown sample format:\", sample)\n",
    "\n",
    "def evaluate_dataset(model, dataset, dataset_name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    layers_used = []\n",
    "    total_tokens = 0\n",
    "    uncertainties = []     # NEW\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for sample in dataset:\n",
    "        text, label = extract_text_label(sample)\n",
    "\n",
    "        # Bayesian model returns 4 outputs\n",
    "        # Baseline model: pred, layer, conf, unc=None  (you already fixed baseline)\n",
    "        pred, layer, conf, unc = model.classify(text)\n",
    "\n",
    "        correct += (pred == label)\n",
    "        total += 1\n",
    "        layers_used.append(layer)\n",
    "        uncertainties.append(unc if unc is not None else 0.0)\n",
    "\n",
    "        total_tokens += len(model.tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "    end = time.time()\n",
    "    latency = (end - start) / total\n",
    "\n",
    "    return {\n",
    "        \"metric\": \"accuracy\",\n",
    "        \"score\": correct / total,\n",
    "        \"avg_latency_sec\": latency,\n",
    "        \"tokens_per_sec\": total_tokens / (end - start),\n",
    "        \"avg_layers_used\": float(np.mean(layers_used)),\n",
    "        \"avg_uncertainty\": float(np.mean(uncertainties)),   # NEW\n",
    "        \"num_samples\": total\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f55be5-411f-4909-8554-a5456a36cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BASELINE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing BASELINE on sst2...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.508, 'avg_latency_sec': 0.04796639823913574, 'tokens_per_sec': 542.6715566640596, 'avg_layers_used': 21.0, 'avg_uncertainty': 0.0, 'num_samples': 500}\n",
      "\n",
      "Testing BASELINE on imdb...\n",
      "imdb {'metric': 'accuracy', 'score': 0.528, 'avg_latency_sec': 0.19430025577545165, 'tokens_per_sec': 1488.9944372196362, 'avg_layers_used': 21.0, 'avg_uncertainty': 0.0, 'num_samples': 500}\n",
      "\n",
      "Testing BASELINE on amazon_polarity...\n",
      "amazon_polarity {'metric': 'accuracy', 'score': 0.496, 'avg_latency_sec': 0.07481225967407226, 'tokens_per_sec': 1291.7134226529881, 'avg_layers_used': 21.0, 'avg_uncertainty': 0.0, 'num_samples': 500}\n"
     ]
    }
   ],
   "source": [
    "results_table = []\n",
    "\n",
    "print(\"Running BASELINE\")\n",
    "\n",
    "baseline_model = ModernBERTBaselineClassifier()\n",
    "\n",
    "for name, meta in cached_datasets.items():\n",
    "    dataset = meta[\"data\"]\n",
    "    print(f\"\\nTesting BASELINE on {name}...\")\n",
    "\n",
    "    result = evaluate_dataset(baseline_model, dataset, name)\n",
    "    print(name, result)\n",
    "\n",
    "    results_table.append({\n",
    "        \"dataset\": name,\n",
    "        \"threshold\": None,\n",
    "        \"mode\": \"baseline\",\n",
    "        \"metric\": result[\"metric\"],\n",
    "        \"score\": float(result[\"score\"]),\n",
    "        \"avg_latency_sec\": float(result[\"avg_latency_sec\"]),\n",
    "        \"tokens_per_sec\": float(result[\"tokens_per_sec\"]),\n",
    "        \"avg_layers_used\": float(result[\"avg_layers_used\"]),\n",
    "        \"num_samples\": int(result[\"num_samples\"]),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b125d-8429-439a-8548-f92d8db9d464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================\n",
      "Testing EXIT LAYERS: [2, 4, 6, 8, 10]\n",
      "=================================================\n",
      "\n",
      "----------------------------\n",
      "Threshold = 0.005\n",
      "Exit Layers = [2, 4, 6, 8, 10]\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing sst2 (exit_layers=[2, 4, 6, 8, 10], threshold=0.005)...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.546, 'avg_latency_sec': 0.004455510139465332, 'tokens_per_sec': 5842.204188794336, 'avg_layers_used': 3.0, 'avg_uncertainty': 2.842170943040401e-17, 'num_samples': 500}\n",
      "\n",
      "Testing imdb (exit_layers=[2, 4, 6, 8, 10], threshold=0.005)...\n",
      "imdb {'metric': 'accuracy', 'score': 0.52, 'avg_latency_sec': 0.01574358606338501, 'tokens_per_sec': 18376.499409677403, 'avg_layers_used': 3.0, 'avg_uncertainty': 2.913225216616411e-16, 'num_samples': 500}\n",
      "\n",
      "Testing amazon_polarity (exit_layers=[2, 4, 6, 8, 10], threshold=0.005)...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# EARLY EXIT NEXT\n",
    "# -------------------------\n",
    "exit_layer_groups = [\n",
    "    [2, 4, 6, 8, 10],\n",
    "    [4, 8],\n",
    "    [3, 6, 9],\n",
    "    [6]\n",
    "]\n",
    "\n",
    "thresholds = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "\n",
    "for exit_layers in exit_layer_groups:\n",
    "\n",
    "    print(\"\\n=================================================\")\n",
    "    print(f\"Testing EXIT LAYERS: {exit_layers}\")\n",
    "    print(\"=================================================\")\n",
    "\n",
    "    for th in thresholds:\n",
    "\n",
    "        print(f\"\\n----------------------------\")\n",
    "        print(f\"Threshold = {th}\")\n",
    "        print(f\"Exit Layers = {exit_layers}\")\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "        # Create modernbert early exit model\n",
    "        model = ModernBERTBayesianEarlyExit(\n",
    "            model_name=\"answerdotai/ModernBERT-base\",\n",
    "            exit_layers=exit_layers,\n",
    "            dropout_passes=10,\n",
    "            uncertainty_threshold=th,\n",
    "            use_entropy=False\n",
    "        )\n",
    "\n",
    "        # Evaluate across datasets\n",
    "        for name, meta in cached_datasets.items():\n",
    "            dataset = meta[\"data\"]\n",
    "\n",
    "            print(f\"\\nTesting {name} (exit_layers={exit_layers}, threshold={th})...\")\n",
    "\n",
    "            result = evaluate_dataset(model, dataset, name)\n",
    "            print(name, result)\n",
    "\n",
    "            # Save results with exit_layers column added\n",
    "            results_table.append({\n",
    "                \"dataset\": name,\n",
    "                \"threshold\": th,\n",
    "                \"exit_layers\": str(exit_layers),\n",
    "                \"mode\": \"early_exit\",\n",
    "                \"metric\": result[\"metric\"],\n",
    "                \"score\": float(result[\"score\"]),\n",
    "                \"tokens_per_sec\": float(result[\"tokens_per_sec\"]),\n",
    "                \"avg_latency_sec\": float(result[\"avg_latency_sec\"]),\n",
    "                \"avg_layers_used\": float(result[\"avg_layers_used\"]),\n",
    "                \"avg_uncertainty\": float(result[\"avg_uncertainty\"]),\n",
    "                \"num_samples\": int(result[\"num_samples\"]),\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728d549-eb48-4e26-ae24-ab7ecdd40a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(results_table)\n",
    "df_sorted = (\n",
    "    df.groupby(\"dataset\", group_keys=True)\n",
    "      .apply(lambda g: g.sort_values(\"score\", ascending=False))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a882082-a4ff-4dee-91c6-c16eca1e9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "df2 = df.copy()\n",
    "\n",
    "# Replace NaN thresholds with 'baseline'\n",
    "df2['threshold'] = df2['threshold'].apply(\n",
    "    lambda x: \"baseline\" if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "# Convert exit_layers list → string for grouping\n",
    "df2['exit_layers_str'] = df2['exit_layers'].apply(lambda x: str(x))\n",
    "\n",
    "datasets = df2['dataset'].unique()\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    df_ds = df2[df2['dataset'] == ds]\n",
    "\n",
    "    # Get all exit_layers configs EXCEPT baseline\n",
    "    exit_configs = sorted(df_ds['exit_layers_str'].unique())\n",
    "    exit_configs = [cfg for cfg in exit_configs if cfg != \"nan\"]\n",
    "\n",
    "    # Only keep configs that are real lists (not baseline)\n",
    "    # ensure exactly 4 for plotting (or fewer → still works)\n",
    "    exit_configs = exit_configs[:4]\n",
    "\n",
    "    # Prepare 2×2 subplot grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Extract baseline row(s)\n",
    "    df_base = df_ds[df_ds['mode'] == \"baseline\"]\n",
    "\n",
    "    # ---------- LOOP OVER EXIT CONFIGS ----------\n",
    "    for idx, exit_cfg in enumerate(exit_configs):\n",
    "\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Subset for this exit layers\n",
    "        df_sub = df_ds[df_ds['exit_layers_str'] == exit_cfg]\n",
    "\n",
    "        ###########################################\n",
    "        # 1) Extract values (early-exit run)\n",
    "        ###########################################\n",
    "        # Early exit values\n",
    "        thresholds = df_sub['threshold'].tolist()\n",
    "        scores     = df_sub['score'].tolist()\n",
    "        layers     = df_sub['avg_layers_used'].tolist()\n",
    "        tps        = df_sub['tokens_per_sec'].tolist()\n",
    "        \n",
    "        # Add baseline explicitly (first element)\n",
    "        if not df_base.empty:\n",
    "            thresholds = [\"baseline\"] + thresholds\n",
    "            scores     = [df_base['score'].iloc[0]] + scores\n",
    "            layers     = [df_base['avg_layers_used'].iloc[0]] + layers\n",
    "            tps        = [df_base['tokens_per_sec'].iloc[0]] + tps\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "        # 2) Extract baseline values\n",
    "        ###########################################\n",
    "        base_thresholds = df_base['threshold'].tolist()\n",
    "        base_scores     = df_base['score'].tolist()\n",
    "        base_layers     = df_base['avg_layers_used'].tolist()\n",
    "        base_tps        = df_base['tokens_per_sec'].tolist()\n",
    "\n",
    "        # ---- Sorting helper: baseline always first ----\n",
    "        def sort_key(x):\n",
    "            return -1 if x == \"baseline\" else float(x)\n",
    "\n",
    "        sorted_idx = sorted(\n",
    "            range(len(thresholds)),\n",
    "            key=lambda i: sort_key(thresholds[i])\n",
    "        )\n",
    "\n",
    "        thresholds = [thresholds[i] for i in sorted_idx]\n",
    "        scores     = [scores[i]     for i in sorted_idx]\n",
    "        layers     = [layers[i]     for i in sorted_idx]\n",
    "        tps        = [tps[i]        for i in sorted_idx]\n",
    "\n",
    "        x_pos = np.arange(len(thresholds))\n",
    "\n",
    "        ################################################\n",
    "        # ---- PLOTS (same style as your original) ----\n",
    "        ################################################\n",
    "        ax1 = ax\n",
    "\n",
    "        # Accuracy curve\n",
    "        ax1.plot(x_pos, scores, marker=\"o\", color=\"tab:blue\", label=\"Accuracy\")\n",
    "        ax1.set_ylabel(\"Accuracy\", color=\"tab:blue\")\n",
    "        ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "        # Plot baseline point in gold\n",
    "        for i, th in enumerate(thresholds):\n",
    "            if th == \"baseline\":\n",
    "                ax1.scatter(\n",
    "                    x_pos[i], scores[i],\n",
    "                    color=\"gold\", s=140, edgecolor=\"black\", zorder=6,\n",
    "                    label=\"Baseline\"\n",
    "                )\n",
    "\n",
    "        # Avg layers\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(\n",
    "            x_pos, layers,\n",
    "            marker=\"s\", linestyle=\"--\",\n",
    "            color=\"tab:red\", label=\"Layers Used\"\n",
    "        )\n",
    "        ax2.set_ylabel(\"Avg Layers Used\", color=\"tab:red\")\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "        # Tokens/sec\n",
    "        ax3 = ax1.twinx()\n",
    "        ax3.spines[\"right\"].set_position((\"outward\", 50))\n",
    "        ax3.plot(\n",
    "            x_pos, tps,\n",
    "            marker=\"^\", linestyle=\":\",\n",
    "            color=\"tab:green\", label=\"Tokens/sec\"\n",
    "        )\n",
    "        ax3.set_ylabel(\"Tokens/sec\", color=\"tab:green\")\n",
    "        ax3.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
    "\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(thresholds)\n",
    "        ax.set_title(f\"exit_layers = {exit_cfg}\")\n",
    "\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    # ------- FIGURE TITLE -------\n",
    "    fig.suptitle(f\"ModernBERT with Bayesian Early-Exit Trade-Off Curves: {ds}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
