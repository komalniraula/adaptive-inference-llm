{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fd9c1c-73d8-4269-b450-ab423498b9d2",
   "metadata": {},
   "source": [
    "### Not finished - working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5325138-3b0e-463d-8ef8-4e4e2a35b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "label_names = {\n",
    "    \"sst2\": [\"negative\", \"positive\"],\n",
    "    \"agnews\": [\"world\", \"sports\", \"business\", \"tech\"]\n",
    "}\n",
    "\n",
    "class GPT2EarlyExitClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, exit_layers, threshold, verbalizers, use_prompt):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.exit_layers = exit_layers\n",
    "        self.threshold = threshold\n",
    "        self.verbalizers = verbalizers\n",
    "        self.num_layers = len(self.model.transformer.h)\n",
    "        self.use_prompt = use_prompt\n",
    "\n",
    "        # Precompute verbalizer token ids\n",
    "        self.verbalizer_token_ids = {}\n",
    "        for dataset, class_map in verbalizers.items():\n",
    "            ids = {}\n",
    "            for cls, words in class_map.items():\n",
    "                tok_lists = [self.tokenizer.encode(\" \" + w) for w in words]\n",
    "                ids[cls] = tok_lists\n",
    "            self.verbalizer_token_ids[dataset] = ids\n",
    "\n",
    "    # Prompt builder\n",
    "    def build_prompt(self, text, dataset_name):\n",
    "        if not self.use_prompt:\n",
    "            return text\n",
    "\n",
    "        if dataset_name == \"sst2\":\n",
    "            return (\n",
    "                \"To which category does the text belong?\\n\"\n",
    "                \"\\\"positive sentiment\\\", \\\"negative sentiment\\\"\\n\\n\"\n",
    "                f\"Text: {text}\\n\"\n",
    "            )\n",
    "\n",
    "        if dataset_name == \"agnews\":\n",
    "            return (\n",
    "                \"What is the topic of this article?\\n\"\n",
    "                \"\\\"world\\\", \\\"sports\\\", \\\"business\\\", \\\"technology\\\"\\n\\n\"\n",
    "                f\"News Article: {text}\\n\"\n",
    "            )\n",
    "\n",
    "        return text\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, text, dataset_name):\n",
    "        labels = label_names[dataset_name]\n",
    "        prompt = self.build_prompt(text, dataset_name)\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        outputs = self.model(input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        class_verbalizers = self.verbalizer_token_ids[dataset_name]\n",
    "\n",
    "        # ---- EARLY EXIT LOOP ----\n",
    "        for layer_idx in self.exit_layers:\n",
    "            h = hidden_states[layer_idx][:, -1, :]\n",
    "            logits = self.model.lm_head(h)[0]\n",
    "\n",
    "            class_scores = []\n",
    "            for cls, tok_lists in class_verbalizers.items():\n",
    "                vals = [logits[torch.tensor(toks)].mean().item() for toks in tok_lists]\n",
    "                class_scores.append(float(np.mean(vals)))\n",
    "\n",
    "            class_scores = torch.tensor(class_scores)\n",
    "            probs = torch.softmax(class_scores, dim=-1)\n",
    "\n",
    "            pred = int(torch.argmax(probs))\n",
    "            conf = float(probs[pred])\n",
    "\n",
    "            if conf >= self.threshold:\n",
    "                return {\n",
    "                    \"layer\": layer_idx,\n",
    "                    \"pred\": labels[pred],\n",
    "                    \"conf\": conf,\n",
    "                    \"scores\": {labels[i]: float(class_scores[i]) for i in range(len(labels))},\n",
    "                    \"probs\": {labels[i]: float(probs[i]) for i in range(len(labels))}\n",
    "                }\n",
    "\n",
    "        # ---- FINAL EXIT AT LAYER 12 ----\n",
    "        final_layer = self.num_layers\n",
    "        h = hidden_states[final_layer][:, -1, :]\n",
    "        logits = self.model.lm_head(h)[0]\n",
    "\n",
    "        class_scores = []\n",
    "        for cls, tok_lists in class_verbalizers.items():\n",
    "            vals = [logits[torch.tensor(toks)].mean().item() for toks in tok_lists]\n",
    "            class_scores.append(float(np.mean(vals)))\n",
    "\n",
    "        class_scores = torch.tensor(class_scores)\n",
    "        probs = torch.softmax(class_scores, dim=-1)\n",
    "        pred = int(torch.argmax(probs))\n",
    "\n",
    "        return {\n",
    "            \"layer\": final_layer,\n",
    "            \"pred\": labels[pred],\n",
    "            \"conf\": float(probs[pred]),\n",
    "            \"scores\": {labels[i]: float(class_scores[i]) for i in range(len(labels))},\n",
    "            \"probs\": {labels[i]: float(probs[i]) for i in range(len(labels))}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c307e9-468a-447d-8668-05cda252eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.dataset_loaders.sst2 import load_sst2\n",
    "from evaluation.dataset_loaders.agnews import load_agnews\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "verbalizers = {\n",
    "    \"sst2\": {\n",
    "        0: [\"negative\"],\n",
    "        1: [\"positive\"]\n",
    "    },\n",
    "    \"agnews\": {\n",
    "        0: [\"international\", \"world\", \"global\"],\n",
    "        1: [\"sports\", \"sport\"],\n",
    "        2: [\"business\", \"finance\", \"market\"],\n",
    "        3: [\"technology\", \"tech\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_loaders = [\n",
    "    (\"sst2\", load_sst2),\n",
    "    (\"agnews\", load_agnews),\n",
    "]\n",
    "\n",
    "\n",
    "def extract(sample):\n",
    "    if isinstance(sample, dict):\n",
    "        for k in [\"text\", \"sentence\", \"input_text\"]:\n",
    "            if k in sample:\n",
    "                return sample[k], sample[\"label\"]\n",
    "    return sample[0], sample[1]\n",
    "\n",
    "def run_experiment(use_prompt, threshold, samples):\n",
    "\n",
    "    model = GPT2EarlyExitClassifier(\n",
    "        model_name=\"gpt2\",\n",
    "        exit_layers=[2,4,6,8,10],\n",
    "        threshold=threshold,\n",
    "        verbalizers=verbalizers,\n",
    "        use_prompt=use_prompt\n",
    "    )\n",
    "\n",
    "    records = []\n",
    "    summary_records = []\n",
    "\n",
    "    for name, loader in dataset_loaders:\n",
    "        dataset = loader(number=samples)\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        layers = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        for sample in dataset:\n",
    "            text, gold = extract(sample)\n",
    "            out = model.classify(text, name)\n",
    "\n",
    "            gold_name = label_names[name][gold]\n",
    "\n",
    "            # per-sample record\n",
    "            row = {\n",
    "                \"dataset\": name,\n",
    "                \"text\": text,\n",
    "                \"gold\": gold,\n",
    "                \"gold_name\": gold_name,\n",
    "                \"pred\": out[\"pred\"],\n",
    "                \"layer\": out[\"layer\"],\n",
    "                \"conf\": out[\"conf\"],\n",
    "            }\n",
    "\n",
    "            # add class-wise scores\n",
    "            for label, sc in out[\"scores\"].items():\n",
    "                row[f\"{label}_score\"] = sc\n",
    "            for label, sc in out[\"probs\"].items():\n",
    "                row[f\"{label}_prob\"] = sc\n",
    "\n",
    "            records.append(row)\n",
    "\n",
    "            # summary stats\n",
    "            layers.append(out[\"layer\"])\n",
    "            correct += (out[\"pred\"] == gold_name)\n",
    "            total += 1\n",
    "            total_tokens += len(model.tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        summary = {\n",
    "            \"dataset\": name,\n",
    "            \"mode\": \"with_prompt\" if use_prompt else \"without_prompt\",\n",
    "            \"threshold\": threshold,\n",
    "            \"metric\": \"accuracy\",\n",
    "            \"score\": correct/total,\n",
    "            \"avg_latency_sec\": (t1-t0)/total,\n",
    "            \"tokens_per_sec\": total_tokens/(t1-t0),\n",
    "            \"avg_layers_used\": float(np.mean(layers)),\n",
    "            \"num_samples\": total\n",
    "        }\n",
    "\n",
    "        summary_records.append(summary)\n",
    "\n",
    "    return pd.DataFrame(records), pd.DataFrame(summary_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61123719-df2c-4c39-94dd-3e7c649ef344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>mode</th>\n",
       "      <th>threshold</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "      <th>avg_latency_sec</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>avg_layers_used</th>\n",
       "      <th>num_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sst2</td>\n",
       "      <td>with_prompt</td>\n",
       "      <td>0.8</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.030532</td>\n",
       "      <td>772.110025</td>\n",
       "      <td>2.708</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agnews</td>\n",
       "      <td>with_prompt</td>\n",
       "      <td>0.8</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.038963</td>\n",
       "      <td>1325.457028</td>\n",
       "      <td>3.296</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset         mode  threshold    metric  score  avg_latency_sec  \\\n",
       "0    sst2  with_prompt        0.8  accuracy  0.512         0.030532   \n",
       "1  agnews  with_prompt        0.8  accuracy  0.380         0.038963   \n",
       "\n",
       "   tokens_per_sec  avg_layers_used  num_samples  \n",
       "0      772.110025            2.708          500  \n",
       "1     1325.457028            3.296          500  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = 0.8\n",
    "sam = 500\n",
    "df_with_prompt, df_sum_with_prompt = run_experiment(use_prompt=True, threshold=th, samples=sam)\n",
    "df_without_prompt, df_sum_without_prompt = run_experiment(use_prompt=False, threshold=th, samples=sam)\n",
    "\n",
    "df_with_prompt.head()\n",
    "df_sum_with_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edc038f2-43be-4f11-905b-b85f560d76d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>mode</th>\n",
       "      <th>threshold</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "      <th>avg_latency_sec</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>avg_layers_used</th>\n",
       "      <th>num_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sst2</td>\n",
       "      <td>without_prompt</td>\n",
       "      <td>0.8</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>887.840841</td>\n",
       "      <td>6.156</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agnews</td>\n",
       "      <td>without_prompt</td>\n",
       "      <td>0.8</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.033115</td>\n",
       "      <td>1559.544666</td>\n",
       "      <td>2.472</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset            mode  threshold    metric  score  avg_latency_sec  \\\n",
       "0    sst2  without_prompt        0.8  accuracy  0.614         0.026552   \n",
       "1  agnews  without_prompt        0.8  accuracy  0.310         0.033115   \n",
       "\n",
       "   tokens_per_sec  avg_layers_used  num_samples  \n",
       "0      887.840841            6.156          500  \n",
       "1     1559.544666            2.472          500  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum_without_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eefce2e-9b07-4b55-88e2-d5bb3df377d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "      <th>gold_name</th>\n",
       "      <th>pred</th>\n",
       "      <th>layer</th>\n",
       "      <th>conf</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_prob</th>\n",
       "      <th>positive_prob</th>\n",
       "      <th>world_score</th>\n",
       "      <th>sports_score</th>\n",
       "      <th>business_score</th>\n",
       "      <th>tech_score</th>\n",
       "      <th>world_prob</th>\n",
       "      <th>sports_prob</th>\n",
       "      <th>business_prob</th>\n",
       "      <th>tech_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sst2</td>\n",
       "      <td>a science-fiction pastiche so lacking in origi...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>12</td>\n",
       "      <td>0.658594</td>\n",
       "      <td>-79.474403</td>\n",
       "      <td>-80.131439</td>\n",
       "      <td>0.658594</td>\n",
       "      <td>0.341406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sst2</td>\n",
       "      <td>it haunts you , you ca n't forget it , you adm...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>6</td>\n",
       "      <td>0.950830</td>\n",
       "      <td>-4.314972</td>\n",
       "      <td>-1.352921</td>\n",
       "      <td>0.049170</td>\n",
       "      <td>0.950830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sst2</td>\n",
       "      <td>nicks , seemingly uncertain what 's going to m...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>6</td>\n",
       "      <td>0.809370</td>\n",
       "      <td>-3.339309</td>\n",
       "      <td>-1.893386</td>\n",
       "      <td>0.190630</td>\n",
       "      <td>0.809370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sst2</td>\n",
       "      <td>if there 's one thing this world needs less of...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>10</td>\n",
       "      <td>0.801071</td>\n",
       "      <td>-24.434387</td>\n",
       "      <td>-25.827389</td>\n",
       "      <td>0.801071</td>\n",
       "      <td>0.198929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sst2</td>\n",
       "      <td>chokes on its own depiction of upper-crust dec...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816541</td>\n",
       "      <td>-22.957804</td>\n",
       "      <td>-24.450891</td>\n",
       "      <td>0.816541</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>agnews</td>\n",
       "      <td>South Koreans Say Secret Work Refined Uranium ...</td>\n",
       "      <td>0</td>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>2</td>\n",
       "      <td>0.947984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.752152</td>\n",
       "      <td>-13.032355</td>\n",
       "      <td>-11.453743</td>\n",
       "      <td>-9.877625</td>\n",
       "      <td>0.947984</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.008608</td>\n",
       "      <td>0.041632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>agnews</td>\n",
       "      <td>Testaverde accepts Parcells #39; nomination Wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "      <td>world</td>\n",
       "      <td>2</td>\n",
       "      <td>0.993671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000196</td>\n",
       "      <td>-7.019704</td>\n",
       "      <td>-11.387556</td>\n",
       "      <td>-6.545090</td>\n",
       "      <td>0.993671</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.003883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>agnews</td>\n",
       "      <td>Gray, Demon Deacons Swing Back In Action, Seek...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "      <td>world</td>\n",
       "      <td>2</td>\n",
       "      <td>0.975014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.519638</td>\n",
       "      <td>-13.623807</td>\n",
       "      <td>-11.563251</td>\n",
       "      <td>-10.517523</td>\n",
       "      <td>0.975014</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.017896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>agnews</td>\n",
       "      <td>Still no beef resolution after latest talks NE...</td>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>world</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.210962</td>\n",
       "      <td>-7.580405</td>\n",
       "      <td>-11.728655</td>\n",
       "      <td>-7.747224</td>\n",
       "      <td>0.996820</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>agnews</td>\n",
       "      <td>Dream TV Screen, Now in Size Large The most de...</td>\n",
       "      <td>3</td>\n",
       "      <td>tech</td>\n",
       "      <td>tech</td>\n",
       "      <td>4</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.001536</td>\n",
       "      <td>-10.026829</td>\n",
       "      <td>-11.186140</td>\n",
       "      <td>-3.899918</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.991124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset                                               text  gold  \\\n",
       "0      sst2  a science-fiction pastiche so lacking in origi...     0   \n",
       "1      sst2  it haunts you , you ca n't forget it , you adm...     1   \n",
       "2      sst2  nicks , seemingly uncertain what 's going to m...     0   \n",
       "3      sst2  if there 's one thing this world needs less of...     0   \n",
       "4      sst2  chokes on its own depiction of upper-crust dec...     0   \n",
       "..      ...                                                ...   ...   \n",
       "995  agnews  South Koreans Say Secret Work Refined Uranium ...     0   \n",
       "996  agnews  Testaverde accepts Parcells #39; nomination Wh...     1   \n",
       "997  agnews  Gray, Demon Deacons Swing Back In Action, Seek...     1   \n",
       "998  agnews  Still no beef resolution after latest talks NE...     2   \n",
       "999  agnews  Dream TV Screen, Now in Size Large The most de...     3   \n",
       "\n",
       "    gold_name      pred  layer      conf  negative_score  positive_score  \\\n",
       "0    negative  negative     12  0.658594      -79.474403      -80.131439   \n",
       "1    positive  positive      6  0.950830       -4.314972       -1.352921   \n",
       "2    negative  positive      6  0.809370       -3.339309       -1.893386   \n",
       "3    negative  negative     10  0.801071      -24.434387      -25.827389   \n",
       "4    negative  negative     10  0.816541      -22.957804      -24.450891   \n",
       "..        ...       ...    ...       ...             ...             ...   \n",
       "995     world     world      2  0.947984             NaN             NaN   \n",
       "996    sports     world      2  0.993671             NaN             NaN   \n",
       "997    sports     world      2  0.975014             NaN             NaN   \n",
       "998  business     world      2  0.996820             NaN             NaN   \n",
       "999      tech      tech      4  0.991124             NaN             NaN   \n",
       "\n",
       "     negative_prob  positive_prob  world_score  sports_score  business_score  \\\n",
       "0         0.658594       0.341406          NaN           NaN             NaN   \n",
       "1         0.049170       0.950830          NaN           NaN             NaN   \n",
       "2         0.190630       0.809370          NaN           NaN             NaN   \n",
       "3         0.801071       0.198929          NaN           NaN             NaN   \n",
       "4         0.816541       0.183459          NaN           NaN             NaN   \n",
       "..             ...            ...          ...           ...             ...   \n",
       "995            NaN            NaN    -6.752152    -13.032355      -11.453743   \n",
       "996            NaN            NaN    -1.000196     -7.019704      -11.387556   \n",
       "997            NaN            NaN    -6.519638    -13.623807      -11.563251   \n",
       "998            NaN            NaN    -1.210962     -7.580405      -11.728655   \n",
       "999            NaN            NaN    -9.001536    -10.026829      -11.186140   \n",
       "\n",
       "     tech_score  world_prob  sports_prob  business_prob  tech_prob  \n",
       "0           NaN         NaN          NaN            NaN        NaN  \n",
       "1           NaN         NaN          NaN            NaN        NaN  \n",
       "2           NaN         NaN          NaN            NaN        NaN  \n",
       "3           NaN         NaN          NaN            NaN        NaN  \n",
       "4           NaN         NaN          NaN            NaN        NaN  \n",
       "..          ...         ...          ...            ...        ...  \n",
       "995   -9.877625    0.947984     0.001776       0.008608   0.041632  \n",
       "996   -6.545090    0.993671     0.002415       0.000031   0.003883  \n",
       "997  -10.517523    0.975014     0.000801       0.006289   0.017896  \n",
       "998   -7.747224    0.996820     0.001708       0.000027   0.001445  \n",
       "999   -3.899918    0.006033     0.002164       0.000679   0.991124  \n",
       "\n",
       "[1000 rows x 19 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be4794-e42d-469c-9e9c-2fa4e7449149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
