{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e6a57c-962d-451c-834f-ed375b0ef7eb",
   "metadata": {},
   "source": [
    "### Not finished - there are issues, still working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5325138-3b0e-463d-8ef8-4e4e2a35b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "class GPT2EarlyExitClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, exit_layers, threshold, verbalizers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.exit_layers = exit_layers            \n",
    "        self.threshold = threshold\n",
    "        self.verbalizers = verbalizers            # {dataset: {class: [words]}}\n",
    "        self.num_layers = len(self.model.transformer.h)\n",
    "\n",
    "        # Precompute token IDs for verbalizers\n",
    "        self.verbalizer_token_ids = {}\n",
    "        for dataset, class_map in verbalizers.items():\n",
    "            ids = {}\n",
    "            for cls, words in class_map.items():\n",
    "                tok_lists = []\n",
    "                for w in words:\n",
    "                    tok_lists.append(self.tokenizer.encode(\" \" + w))\n",
    "                ids[cls] = tok_lists\n",
    "            self.verbalizer_token_ids[dataset] = ids\n",
    "\n",
    "    def build_prompt(self, text, dataset_name):\n",
    "        if dataset_name == \"sst2\":\n",
    "            return (\n",
    "                \"To which category does the text belong?\\n\"\n",
    "                \"\\\"positive sentiment\\\", \\\"negative sentiment\\\"\\n\\n\"\n",
    "                f\"Text: {text}\\n\\n\"\n",
    "            )\n",
    "    \n",
    "        if dataset_name == \"agnews\":\n",
    "            return (\n",
    "                \"What is the topic of this article?\\n\"\n",
    "                \"\\\"world\\\", \\\"sports\\\", \\\"business\\\", \\\"technology\\\"\\n\\n\"\n",
    "                f\"News Article: {text}\\n\\n\"\n",
    "            )\n",
    "    \n",
    "        return text\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, text, dataset_name):\n",
    "        # Insert prompt\n",
    "        prompt = self.build_prompt(text, dataset_name)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "        outputs = self.model(input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        class_verbalizers = self.verbalizer_token_ids[dataset_name]\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # EARLY EXIT AT SPECIFIED INTERMEDIATE LAYERS\n",
    "        # ----------------------------------------\n",
    "        for layer_idx in self.exit_layers:\n",
    "\n",
    "            # last token position (GPT-2 has no padding)\n",
    "            last_idx = input_ids.shape[1] - 1\n",
    "        \n",
    "            h = hidden_states[layer_idx][:, last_idx, :]   # (1, hidden_dim)\n",
    "            logits = self.model.lm_head(h)[0]\n",
    "\n",
    "            # Compute verbalizer class scores\n",
    "            class_scores = []\n",
    "            for cls, tok_lists in class_verbalizers.items():\n",
    "                v_scores = []\n",
    "                for tok_list in tok_lists:\n",
    "                    idx = torch.tensor(tok_list, dtype=torch.long)\n",
    "                    v_scores.append(logits[idx].mean().item())\n",
    "                #class_scores.append(max(v_scores))\n",
    "                class_scores.append(float(np.mean(v_scores)))\n",
    "\n",
    "            class_scores = torch.tensor(class_scores)\n",
    "            class_probs = torch.softmax(class_scores, dim=-1)\n",
    "\n",
    "            pred = int(torch.argmax(class_probs))\n",
    "            conf = float(class_probs[pred])\n",
    "\n",
    "            if conf >= self.threshold:\n",
    "                return pred, layer_idx, conf   # EARLY EXIT SUCCESS\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # FINAL LAYER EXIT (LAYER 12)\n",
    "        # ALWAYS EXECUTED, NO MATTER WHAT\n",
    "        # ----------------------------------------\n",
    "        final_layer = self.num_layers  # typically 12\n",
    "\n",
    "        # last token position (GPT-2 has no padding)\n",
    "        last_idx = input_ids.shape[1] - 1\n",
    "    \n",
    "        h = hidden_states[layer_idx][:, last_idx, :]   # (1, hidden_dim)\n",
    "        logits = self.model.lm_head(h)[0]\n",
    "\n",
    "        class_scores = []\n",
    "        for cls, tok_lists in class_verbalizers.items():\n",
    "            v_scores = []\n",
    "            for tok_list in tok_lists:\n",
    "                idx = torch.tensor(tok_list, dtype=torch.long)\n",
    "                v_scores.append(logits[idx].mean().item())\n",
    "            #class_scores.append(max(v_scores))\n",
    "            class_scores.append(float(np.mean(v_scores)))\n",
    "\n",
    "        class_scores = torch.tensor(class_scores)\n",
    "        class_probs = torch.softmax(class_scores, dim=-1)\n",
    "\n",
    "        pred = int(torch.argmax(class_probs))\n",
    "        conf = float(class_probs[pred])\n",
    "\n",
    "        # FINAL EXIT — threshold ignored\n",
    "        return pred, final_layer, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c307e9-468a-447d-8668-05cda252eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# BASELINE classifier (FINAL LAYER ONLY)\n",
    "# ---------------------------------------\n",
    "class GPT2BaselineClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, verbalizers):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.verbalizers = verbalizers\n",
    "        self.num_layers = len(self.model.transformer.h)\n",
    "\n",
    "        # Precompute tokens\n",
    "        self.verbalizer_token_ids = {}\n",
    "        for dataset, class_map in verbalizers.items():\n",
    "            ids = {}\n",
    "            for cls, words in class_map.items():\n",
    "                tok_list = self.tokenizer.encode(\" \" + words[0])\n",
    "                ids[cls] = tok_list\n",
    "            self.verbalizer_token_ids[dataset] = ids\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, text, dataset_name):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        outputs = self.model(input_ids, output_hidden_states=True)\n",
    "        hidden = outputs.hidden_states[self.num_layers][:, -1, :]\n",
    "\n",
    "        logits = self.model.lm_head(hidden)[0]\n",
    "\n",
    "        class_verbalizers = self.verbalizer_token_ids[dataset_name]\n",
    "        scores = []\n",
    "\n",
    "        for cls, tok_list in class_verbalizers.items():\n",
    "            idx = torch.tensor(tok_list, dtype=torch.long)\n",
    "            scores.append(logits[idx].mean().item())\n",
    "\n",
    "        scores = torch.tensor(scores)\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        pred = int(torch.argmax(probs))\n",
    "        return pred, self.num_layers, float(probs[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b309041-0abc-4df0-90f9-5f20530679ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0.9013482332229614\n"
     ]
    }
   ],
   "source": [
    "verbalizers = {\n",
    "    \"sst2\": {\n",
    "        0: [\"negative\"],\n",
    "        1: [\"positive\"]\n",
    "    },\n",
    "    \"agnews\": {\n",
    "        0: [\"international\", \"world\", \"global\"],\n",
    "        1: [\"sports\", \"sport\"],\n",
    "        2: [\"business\", \"finance\", \"market\"],\n",
    "        3: [\"technology\", \"tech\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "model = GPT2EarlyExitClassifier(\n",
    "    model_name=\"gpt2\",\n",
    "    exit_layers=[2,4,6,8,10,12],\n",
    "    threshold=0.8,\n",
    "    verbalizers=verbalizers\n",
    ")\n",
    "\n",
    "pred, layer_used, conf = model.classify(\"This movie was great!\", dataset_name=\"sst2\")\n",
    "print(pred, layer_used, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19511dec-660a-43ee-9b13-10f80e4c9db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets once...\n",
      "\n",
      "Loading sst2...\n",
      "Loading agnews...\n",
      "\n",
      "All datasets loaded.\n",
      "\n",
      "\n",
      "==============================\n",
      "Running BASELINE\n",
      "==============================\n",
      "\n",
      "Testing BASELINE on sst2...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.582, 'avg_latency_sec': 0.021676305770874024, 'tokens_per_sec': 1087.5469394640054, 'avg_layers_used': 12.0, 'num_samples': 500}\n",
      "\n",
      "Testing BASELINE on agnews...\n",
      "agnews {'metric': 'accuracy', 'score': 0.564, 'avg_latency_sec': 0.0287554874420166, 'tokens_per_sec': 1795.9702510394393, 'avg_layers_used': 12.0, 'num_samples': 500}\n",
      "\n",
      "==============================\n",
      "Running threshold 0.5\n",
      "==============================\n",
      "\n",
      "Testing sst2 (early_exit threshold=0.5)...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.512, 'avg_latency_sec': 0.027740146160125732, 'tokens_per_sec': 849.8152772491791, 'avg_layers_used': 2.0, 'num_samples': 500}\n",
      "\n",
      "Testing agnews (early_exit threshold=0.5)...\n",
      "agnews {'metric': 'accuracy', 'score': 0.266, 'avg_latency_sec': 0.03765249967575073, 'tokens_per_sec': 1371.5955233978843, 'avg_layers_used': 2.0, 'num_samples': 500}\n",
      "\n",
      "==============================\n",
      "Running threshold 0.6\n",
      "==============================\n",
      "\n",
      "Testing sst2 (early_exit threshold=0.6)...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.512, 'avg_latency_sec': 0.027348013877868654, 'tokens_per_sec': 862.0004401517886, 'avg_layers_used': 2.0, 'num_samples': 500}\n",
      "\n",
      "Testing agnews (early_exit threshold=0.6)...\n",
      "agnews {'metric': 'accuracy', 'score': 0.266, 'avg_latency_sec': 0.03748326635360718, 'tokens_per_sec': 1377.788144523058, 'avg_layers_used': 2.0, 'num_samples': 500}\n",
      "\n",
      "==============================\n",
      "Running threshold 0.7\n",
      "==============================\n",
      "\n",
      "Testing sst2 (early_exit threshold=0.7)...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.512, 'avg_latency_sec': 0.028621135711669923, 'tokens_per_sec': 823.6570427353093, 'avg_layers_used': 2.008, 'num_samples': 500}\n",
      "\n",
      "Testing agnews (early_exit threshold=0.7)...\n",
      "agnews {'metric': 'accuracy', 'score': 0.266, 'avg_latency_sec': 0.03858283996582031, 'tokens_per_sec': 1338.5225153397282, 'avg_layers_used': 2.0, 'num_samples': 500}\n",
      "\n",
      "==============================\n",
      "Running threshold 0.8\n",
      "==============================\n",
      "\n",
      "Testing sst2 (early_exit threshold=0.8)...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.512, 'avg_latency_sec': 0.030207332134246825, 'tokens_per_sec': 780.406554780571, 'avg_layers_used': 2.6, 'num_samples': 500}\n",
      "\n",
      "Testing agnews (early_exit threshold=0.8)...\n",
      "agnews {'metric': 'accuracy', 'score': 0.266, 'avg_latency_sec': 0.042258530616760256, 'tokens_per_sec': 1222.0964441086685, 'avg_layers_used': 2.0, 'num_samples': 500}\n",
      "\n",
      "==============================\n",
      "Running threshold 0.9\n",
      "==============================\n",
      "\n",
      "Testing sst2 (early_exit threshold=0.9)...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.512, 'avg_latency_sec': 0.033301698207855224, 'tokens_per_sec': 707.8918274035452, 'avg_layers_used': 4.512, 'num_samples': 500}\n",
      "\n",
      "Testing agnews (early_exit threshold=0.9)...\n",
      "agnews {'metric': 'accuracy', 'score': 0.268, 'avg_latency_sec': 0.03783558607101441, 'tokens_per_sec': 1364.9583728680268, 'avg_layers_used': 2.144, 'num_samples': 500}\n",
      "\n",
      "==============================\n",
      "Running threshold 1.0\n",
      "==============================\n",
      "\n",
      "Testing sst2 (early_exit threshold=1.0)...\n",
      "sst2 {'metric': 'accuracy', 'score': 0.512, 'avg_latency_sec': 0.03860471391677856, 'tokens_per_sec': 610.6508145823652, 'avg_layers_used': 11.996, 'num_samples': 500}\n",
      "\n",
      "Testing agnews (early_exit threshold=1.0)...\n",
      "agnews {'metric': 'accuracy', 'score': 0.266, 'avg_latency_sec': 0.048114058494567874, 'tokens_per_sec': 1073.3661141021946, 'avg_layers_used': 10.932, 'num_samples': 500}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from evaluation.dataset_loaders.sst2 import load_sst2\n",
    "from evaluation.dataset_loaders.agnews import load_agnews\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Verbalyzers for GPT-2 classification\n",
    "# ---------------------------------------\n",
    "verbalizers = {\n",
    "    \"sst2\": {\n",
    "        0: [\"negative\"],\n",
    "        1: [\"positive\"],\n",
    "    },\n",
    "    \"agnews\": {\n",
    "        0: [\"international\", \"world\", \"global\"],\n",
    "        1: [\"sports\", \"sport\"],\n",
    "        2: [\"business\", \"finance\", \"market\"],\n",
    "        3: [\"technology\", \"tech\", \"computer\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load datasets once\n",
    "# ---------------------------------------\n",
    "dataset_loaders = [\n",
    "    (\"sst2\", load_sst2, \"classification\"),\n",
    "    (\"agnews\", load_agnews, \"classification\"),\n",
    "]\n",
    "\n",
    "cached_datasets = {}\n",
    "print(\"Loading datasets once...\\n\")\n",
    "\n",
    "for name, loader, task in dataset_loaders:\n",
    "    print(f\"Loading {name}...\")\n",
    "    cached_datasets[name] = {\n",
    "        \"data\": loader(number=500),\n",
    "        \"task\": task\n",
    "    }\n",
    "\n",
    "print(\"\\nAll datasets loaded.\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Extract (text, label) from any format\n",
    "# ---------------------------------------\n",
    "def extract_text_label(sample):\n",
    "    if isinstance(sample, dict):\n",
    "        if \"text\" in sample:\n",
    "            return sample[\"text\"], sample[\"label\"]\n",
    "        elif \"sentence\" in sample:\n",
    "            return sample[\"sentence\"], sample[\"label\"]\n",
    "        elif \"input_text\" in sample:\n",
    "            return sample[\"input_text\"], sample[\"label\"]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown dict format:\", sample)\n",
    "\n",
    "    if isinstance(sample, (tuple, list)):\n",
    "        return sample[0], sample[1]\n",
    "\n",
    "    raise ValueError(\"Unknown sample format:\", sample)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Evaluation\n",
    "# ---------------------------------------\n",
    "def evaluate_dataset(model, dataset, dataset_name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    layers_used = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for sample in dataset:\n",
    "        text, label = extract_text_label(sample)\n",
    "\n",
    "        pred, layer, conf = model.classify(text, dataset_name)\n",
    "\n",
    "        correct += (pred == label)\n",
    "        total += 1\n",
    "        layers_used.append(layer)\n",
    "\n",
    "        total_tokens += len(model.tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "    end = time.time()\n",
    "    latency = (end - start) / total\n",
    "\n",
    "    return {\n",
    "        \"metric\": \"accuracy\",\n",
    "        \"score\": correct / total,\n",
    "        \"avg_latency_sec\": latency,\n",
    "        \"tokens_per_sec\": total_tokens / (end - start),\n",
    "        \"avg_layers_used\": float(np.mean(layers_used)),\n",
    "        \"num_samples\": total\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Run baseline + early exit\n",
    "# ---------------------------------------\n",
    "results_table = []\n",
    "\n",
    "# -------------------------\n",
    "# BASELINE FIRST\n",
    "# -------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"Running BASELINE\")\n",
    "print(\"==============================\")\n",
    "\n",
    "baseline_model = GPT2BaselineClassifier(\n",
    "    model_name=\"gpt2\",\n",
    "    verbalizers=verbalizers\n",
    ")\n",
    "\n",
    "for name, meta in cached_datasets.items():\n",
    "    dataset = meta[\"data\"]\n",
    "    print(f\"\\nTesting BASELINE on {name}...\")\n",
    "\n",
    "    result = evaluate_dataset(baseline_model, dataset, name)\n",
    "    print(name, result)\n",
    "\n",
    "    results_table.append({\n",
    "        \"dataset\": name,\n",
    "        \"threshold\": None,\n",
    "        \"mode\": \"baseline\",\n",
    "        \"metric\": result[\"metric\"],\n",
    "        \"score\": float(result[\"score\"]),\n",
    "        \"avg_latency_sec\": float(result[\"avg_latency_sec\"]),\n",
    "        \"tokens_per_sec\": float(result[\"tokens_per_sec\"]),\n",
    "        \"avg_layers_used\": float(result[\"avg_layers_used\"]),\n",
    "        \"num_samples\": int(result[\"num_samples\"]),\n",
    "    })\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# EARLY EXIT NEXT\n",
    "# -------------------------\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "exit_layers = [2, 4, 6, 8, 10]\n",
    "\n",
    "for th in thresholds:\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Running threshold {th}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    model = GPT2EarlyExitClassifier(\n",
    "        model_name=\"gpt2\",\n",
    "        exit_layers=exit_layers,\n",
    "        threshold=th,\n",
    "        verbalizers=verbalizers\n",
    "    )\n",
    "\n",
    "    for name, meta in cached_datasets.items():\n",
    "        dataset = meta[\"data\"]\n",
    "\n",
    "        print(f\"\\nTesting {name} (early_exit threshold={th})...\")\n",
    "\n",
    "        result = evaluate_dataset(model, dataset, name)\n",
    "        print(name, result)\n",
    "\n",
    "        results_table.append({\n",
    "            \"dataset\": name,\n",
    "            \"threshold\": th,\n",
    "            \"mode\": \"early_exit\",\n",
    "            \"metric\": result[\"metric\"],\n",
    "            \"score\": float(result[\"score\"]),\n",
    "            \"avg_latency_sec\": float(result[\"avg_latency_sec\"]),\n",
    "            \"tokens_per_sec\": float(result[\"tokens_per_sec\"]),\n",
    "            \"avg_layers_used\": float(result[\"avg_layers_used\"]),\n",
    "            \"num_samples\": int(result[\"num_samples\"]),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092156b-f57b-467c-add2-f39c06b189bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(results_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30288ecd-02b9-4b2b-b116-77cc4a414d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "grouped = {}\n",
    "\n",
    "# Build structured dataset including baseline\n",
    "for row in results_table:\n",
    "    ds = row[\"dataset\"]\n",
    "    if ds not in grouped:\n",
    "        grouped[ds] = {\n",
    "            \"thresholds\": [],\n",
    "            \"scores\": [],\n",
    "            \"avg_layers\": [],\n",
    "            \"mode\": [],\n",
    "        }\n",
    "\n",
    "    th = row[\"threshold\"]\n",
    "\n",
    "    # Replace baseline's NaN with the string \"baseline\"\n",
    "    if th is None or (isinstance(th, float) and math.isnan(th)):\n",
    "        th = \"baseline\"\n",
    "\n",
    "    grouped[ds][\"thresholds\"].append(th)\n",
    "    grouped[ds][\"scores\"].append(row[\"score\"])\n",
    "    grouped[ds][\"avg_layers\"].append(row[\"avg_layers_used\"])\n",
    "    grouped[ds][\"mode\"].append(row[\"mode\"])\n",
    "\n",
    "\n",
    "# Plot for each dataset\n",
    "for ds, data in grouped.items():\n",
    "\n",
    "    thresholds = data[\"thresholds\"]\n",
    "    scores     = data[\"scores\"]\n",
    "    layers     = data[\"avg_layers\"]\n",
    "    modes      = data[\"mode\"]\n",
    "\n",
    "    # ---- Sort so \"baseline\" appears first ----\n",
    "    # baseline = string, others = floats → custom sort key\n",
    "    def sort_key(x):\n",
    "        return -1 if x == \"baseline\" else float(x)\n",
    "\n",
    "    sorted_idx = sorted(range(len(thresholds)), key=lambda i: sort_key(thresholds[i]))\n",
    "\n",
    "    thresholds = [thresholds[i] for i in sorted_idx]\n",
    "    scores     = [scores[i] for i in sorted_idx]\n",
    "    layers     = [layers[i] for i in sorted_idx]\n",
    "    modes      = [modes[i] for i in sorted_idx]\n",
    "\n",
    "    # ---- Categorical x-axis ----\n",
    "    x_labels = thresholds\n",
    "    x_pos    = np.arange(len(x_labels))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Accuracy curve\n",
    "    ax1.set_xlabel(\"Confidence Threshold\")\n",
    "    ax1.set_ylabel(\"Accuracy\", color=\"tab:blue\")\n",
    "    ax1.plot(x_pos, scores, marker=\"o\", color=\"tab:blue\", label=\"Accuracy\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    # Highlight baseline point\n",
    "    for i, m in enumerate(modes):\n",
    "        if m == \"baseline\":\n",
    "            ax1.scatter(x_pos[i], scores[i], color=\"gold\", s=140,\n",
    "                        edgecolor=\"black\", zorder=5, label=\"Baseline\")\n",
    "\n",
    "    # Layers curve (right axis)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"Avg Layers Used\", color=\"tab:red\")\n",
    "    ax2.plot(x_pos, layers, marker=\"s\", linestyle=\"--\",\n",
    "             color=\"tab:red\", label=\"Layers Used\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "    # Put the baseline / threshold labels on x-axis\n",
    "    plt.xticks(x_pos, x_labels)\n",
    "\n",
    "    plt.title(f\"Trade-off Curve: {ds} dataset\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61123719-df2c-4c39-94dd-3e7c649ef344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
