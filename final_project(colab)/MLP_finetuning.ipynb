{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Efficient_AI_Project/EarlyExit_Experiments\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_8ZEe6qNeVo",
        "outputId": "37e344e3-63f8-4479-db13-c5783ddf0e09"
      },
      "id": "1_8ZEe6qNeVo",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSIFIER_MODEL_NAME = \"komal/gpt2-medium-classifier_finetuning_24layer\"\n",
        "MODEL_FOLDER_NAME = \"lr5e-05_wd0.01_ep3_drop0.0_lossW0.9-0.8-0.7-0.6-0.5-0.4-0.3-0.2\"\n",
        "CHECKPOINT_PATH = (\n",
        "    f\"{PROJECT_ROOT}/\"\n",
        "    f\"{CLASSIFIER_MODEL_NAME}/\"\n",
        "    f\"{MODEL_FOLDER_NAME}/\"\n",
        "    f\"best_model/pytorch_model.bin\"\n",
        ")\n",
        "\n",
        "assert os.path.exists(CHECKPOINT_PATH), \"❌ Classifier checkpoint not found!\""
      ],
      "metadata": {
        "id": "XSENBjacWGJP"
      },
      "id": "XSENBjacWGJP",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_MODEL_NAME = \"komal/gpt2-medium-MLP\"\n",
        "\n",
        "PERSISTENT_BASE_DIR = f\"{PROJECT_ROOT}/{MLP_MODEL_NAME}\"\n",
        "os.makedirs(PERSISTENT_BASE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "bKiPk5LTWG46"
      },
      "id": "bKiPk5LTWG46",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!git clone https://github.com/komalniraula/adaptive-inference-llm\n",
        "\n",
        "repo_name = 'adaptive-inference-llm' # Must match the folder created by git clone\n",
        "project_path = os.path.join('/content', repo_name)\n",
        "\n",
        "# Append the project root directory to the system path\n",
        "\n",
        "sys.path.append(project_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3VTri4XEm0M",
        "outputId": "2e93b465-2bb2-4957-a9cd-d325024be274"
      },
      "id": "S3VTri4XEm0M",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'adaptive-inference-llm' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "42f35dfa-e61e-456a-a8af-c0aa7959ccfa",
      "metadata": {
        "id": "42f35dfa-e61e-456a-a8af-c0aa7959ccfa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import itertools\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from evaluation.dataset_loaders.sst2 import load_sst2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Load datasets\n",
        "# -----------------------------\n",
        "# Load the labeled SST-2 train split (67k samples)\n",
        "sst2_train_data = load_sst2(task='train', fraction=1)\n",
        "\n",
        "# Load the SST-2 validation split (872 samples) for testing\n",
        "# This is equivalent to your previous sst2_test = ds[\"validation\"]\n",
        "sst2_test_data = load_sst2(task='test', fraction=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh4abEUmAE0V",
        "outputId": "c86e78be-cd43-4ee3-dafc-c406aead9af8"
      },
      "id": "Rh4abEUmAE0V",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Convert datasets to (text,label)\n",
        "# -----------------------------\n",
        "# The external load_sst2 function should already return a list/Dataset\n",
        "# where elements can be converted.\n",
        "# We adapt the conversion based on the expected output of your loader module.\n",
        "\n",
        "def convert_sst2(sample):\n",
        "    # Assuming your module returns samples with 'text' and 'label' keys\n",
        "    # based on the preprocess function in your original loader.\n",
        "    if \"text\" in sample:\n",
        "        return sample[\"text\"], int(sample[\"label\"])\n",
        "    elif \"sentence\" in sample:\n",
        "        # Fallback for the raw SST-2 key\n",
        "        return sample[\"sentence\"], int(sample[\"label\"])\n",
        "    else:\n",
        "         raise ValueError(\"SST-2 sample missing expected keys for conversion.\")\n",
        "\n",
        "# Create data pairs\n",
        "train_pairs = [convert_sst2(x) for x in sst2_train_data]\n",
        "test_pairs  = [convert_sst2(x) for x in sst2_test_data]\n",
        "\n",
        "print(f\"Total train samples: {len(train_pairs)}\")\n",
        "print(f\"Total test samples: {len(test_pairs)}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset class\n",
        "# -----------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.data[idx]\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Dynamic padding collate_fn\n",
        "# -----------------------------\n",
        "def collate_fn(batch):\n",
        "    input_ids = [b[\"input_ids\"] for b in batch]\n",
        "    labels = torch.tensor([b[\"labels\"] for b in batch])\n",
        "\n",
        "    max_len = max(len(ids) for ids in input_ids)\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    padded = torch.full((len(batch), max_len), pad_id)\n",
        "    for i, ids in enumerate(input_ids):\n",
        "        padded[i, :len(ids)] = ids\n",
        "\n",
        "    attention_mask = (padded != tokenizer.eos_token_id).float()\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": padded,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq94D920ABVW",
        "outputId": "b5f2b328-3d31-4cf4-b7af-011af2892517"
      },
      "id": "uq94D920ABVW",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train samples: 67349\n",
            "Total test samples: 872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "685c08e0-67bf-426e-8bad-b7d2d7ac8242",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "685c08e0-67bf-426e-8bad-b7d2d7ac8242",
        "outputId": "f4d383bb-3ecf-4f90-fd2d-c71d7a621f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader batches: 4210\n",
            "Test loader batches: 55\n",
            "torch.Size([16, 30])\n",
            "torch.Size([16, 30])\n",
            "tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Create datasets\n",
        "# -----------------------------\n",
        "train_ds = SentimentDataset(train_pairs, tokenizer)\n",
        "test_ds = SentimentDataset(test_pairs, tokenizer)\n",
        "\n",
        "# -----------------------------\n",
        "# Create dataloaders\n",
        "# -----------------------------\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(\"Train loader batches:\", len(train_loader))\n",
        "print(\"Test loader batches:\", len(test_loader))\n",
        "\n",
        "# Optional: Inspect first batch\n",
        "example = next(iter(train_loader))\n",
        "print(example[\"input_ids\"].shape)\n",
        "print(example[\"attention_mask\"].shape)\n",
        "print(example[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "92a3861d-1488-47b8-8eff-d36eb44f02f9",
      "metadata": {
        "id": "92a3861d-1488-47b8-8eff-d36eb44f02f9"
      },
      "outputs": [],
      "source": [
        "class ExitMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)   # binary: exit vs continue\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.net(x))\n",
        "\n",
        "class GPT2EarlyExitClassifier(nn.Module):\n",
        "    def __init__(self, model_name, exit_layers, hyperparameters):\n",
        "        super().__init__()\n",
        "\n",
        "        # -----------------------------\n",
        "        # Backbone (already fine-tuned)\n",
        "        # -----------------------------\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        self.exit_layers = sorted(exit_layers)\n",
        "        self.hp = hyperparameters\n",
        "        self.num_labels = self.hp.get(\"num_labels\", 2)\n",
        "        dropout_rate = self.hp.get(\"dropout\", 0.0)\n",
        "\n",
        "        hidden_size = self.model.config.hidden_size\n",
        "\n",
        "        # -----------------------------\n",
        "        # Pre-trained exit classifiers\n",
        "        # -----------------------------\n",
        "        self.exit_heads = nn.ModuleDict()\n",
        "        for layer in self.exit_layers:\n",
        "            self.exit_heads[str(layer)] = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(hidden_size, self.num_labels)\n",
        "            )\n",
        "\n",
        "        # -----------------------------\n",
        "        # Predictive gating MLPs\n",
        "        # input = [hidden_state, max_prob, entropy]\n",
        "        # -----------------------------\n",
        "        mlp_input_dim = hidden_size + 2\n",
        "        self.exit_mlps = nn.ModuleDict()\n",
        "\n",
        "        for layer in self.exit_layers:\n",
        "            self.exit_mlps[str(layer)] = ExitMLP(\n",
        "                input_dim=mlp_input_dim,\n",
        "                hidden_dim=self.hp.get(\"mlp_hidden_dim\", 128),\n",
        "                dropout=dropout_rate\n",
        "            )\n",
        "\n",
        "        self.gate_loss_weight = self.hp.get(\"gate_loss_weight\", 1.0)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Freeze backbone + exit heads\n",
        "        # -----------------------------\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        for p in self.exit_heads.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, exit_labels=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            exit_labels: dict[layer] -> binary labels\n",
        "                         1 = EXIT, 0 = CONTINUE\n",
        "\n",
        "        Returns:\n",
        "            gate loss\n",
        "            logits per exit (for analysis)\n",
        "            exit probabilities per exit\n",
        "        \"\"\"\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        logits_dict = {}\n",
        "        exit_probs_dict = {}\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for layer in self.exit_layers:\n",
        "\n",
        "            # [B, seq_len, H] → [B, H]\n",
        "            cls_vec = hidden_states[layer][:, -1, :]\n",
        "\n",
        "            # ---- Pretrained classifier (frozen) ----\n",
        "            logits = self.exit_heads[str(layer)](cls_vec)\n",
        "            logits_dict[layer] = logits\n",
        "\n",
        "            # ---- Confidence features ----\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            max_prob, _ = probs.max(dim=-1, keepdim=True)\n",
        "            entropy = -(probs * torch.log(probs + 1e-8)).sum(\n",
        "                dim=-1, keepdim=True\n",
        "            )\n",
        "\n",
        "            # ---- Gating MLP ----\n",
        "            mlp_input = torch.cat(\n",
        "                [cls_vec, max_prob, entropy],\n",
        "                dim=-1\n",
        "            )\n",
        "\n",
        "            p_exit = self.exit_mlps[str(layer)](mlp_input)\n",
        "            exit_probs_dict[layer] = p_exit\n",
        "\n",
        "            # ---- Gate loss only ----\n",
        "            if exit_labels is not None:\n",
        "                gate_loss = F.binary_cross_entropy(\n",
        "                    p_exit.squeeze(-1),\n",
        "                    exit_labels[layer].float()\n",
        "                )\n",
        "                total_loss += self.gate_loss_weight * gate_loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": total_loss if exit_labels is not None else None,\n",
        "            \"logits\": logits_dict,\n",
        "            \"exit_probs\": exit_probs_dict\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "36b706b3-305c-404f-ba74-2ef0782f9797",
      "metadata": {
        "id": "36b706b3-305c-404f-ba74-2ef0782f9797"
      },
      "outputs": [],
      "source": [
        "hyperparameter_grid = {\n",
        "    # model config (fixed)\n",
        "    \"num_labels\": [2],\n",
        "    \"dropout\": [0.0],\n",
        "\n",
        "    # MLP gate architecture\n",
        "    \"mlp_hidden_dim\": [64, 128],\n",
        "\n",
        "    # gate loss scaling\n",
        "    \"gate_loss_weight\": [1.0],\n",
        "\n",
        "    # training hyperparams (gate-only)\n",
        "    \"learning_rate\": [1e-4, 3e-4, 3e-5],\n",
        "    \"weight_decay\": [0.0],          # often unnecessary for tiny MLPs\n",
        "    \"num_epochs\": [2],\n",
        "    \"batch_size\": [16],\n",
        "\n",
        "    # stability\n",
        "    \"max_grad_norm\": [1.0],\n",
        "\n",
        "    # logging\n",
        "    \"log_every\": [500]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a759cc13-cba0-4574-b7ed-91fea9c2c6c3",
      "metadata": {
        "id": "a759cc13-cba0-4574-b7ed-91fea9c2c6c3"
      },
      "outputs": [],
      "source": [
        "def make_experiment_name(hp):\n",
        "    return (\n",
        "        f\"mlpH{hp['mlp_hidden_dim']}_\"\n",
        "        f\"lr{hp['learning_rate']}_\"\n",
        "        f\"wd{hp['weight_decay']}_\"\n",
        "        f\"ep{hp['num_epochs']}_\"\n",
        "        f\"drop{hp['dropout']}_\"\n",
        "        f\"batch_size{hp['batch_size']}_\"\n",
        "        f\"gateW{hp['gate_loss_weight']}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7b4418b9-cef6-4793-a1b6-204d5221105c",
      "metadata": {
        "id": "7b4418b9-cef6-4793-a1b6-204d5221105c"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_mlp_gates(model, data_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        y_true = batch[\"labels\"]\n",
        "\n",
        "        logits_out = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            exit_labels=None\n",
        "        )\n",
        "\n",
        "        exit_labels = {}\n",
        "        for layer in model.exit_layers:\n",
        "            preds = logits_out[\"logits\"][layer].argmax(dim=-1)\n",
        "            exit_labels[layer] = (preds == y_true).long()\n",
        "\n",
        "        out = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            exit_labels=exit_labels\n",
        "        )\n",
        "\n",
        "        total_loss += out[\"loss\"].item()\n",
        "\n",
        "        for layer in model.exit_layers:\n",
        "            p_exit = out[\"exit_probs\"][layer].squeeze(-1)\n",
        "            pred_exit = (p_exit > 0.5).long()\n",
        "            correct += (pred_exit == exit_labels[layer]).sum().item()\n",
        "            total += exit_labels[layer].numel()\n",
        "\n",
        "    return total_loss / len(data_loader), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d1ef9a9a-9043-42f1-8a68-8973abbb19b2",
      "metadata": {
        "id": "d1ef9a9a-9043-42f1-8a68-8973abbb19b2"
      },
      "outputs": [],
      "source": [
        "def train_mlp_gates(model, train_loader, val_loader, hp, device):\n",
        "\n",
        "    exp_name = make_experiment_name(hp)\n",
        "    epoch_history = []\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.exit_mlps.parameters(),\n",
        "        lr=hp[\"learning_rate\"],\n",
        "        weight_decay=hp[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    exp_dir = os.path.join(PERSISTENT_BASE_DIR, exp_name)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    best_val_gate_loss = float(\"inf\")\n",
        "    best_epoch_metrics = None\n",
        "\n",
        "    for epoch in range(hp[\"num_epochs\"]):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            y_true = batch[\"labels\"]\n",
        "\n",
        "            # -------------------------------\n",
        "            # Generate exit labels (deterministic)\n",
        "            # -------------------------------\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                logits_out = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    exit_labels=None\n",
        "                )\n",
        "\n",
        "                exit_labels = {}\n",
        "                for layer in model.exit_layers:\n",
        "                    preds = logits_out[\"logits\"][layer].argmax(dim=-1)\n",
        "                    exit_labels[layer] = (preds == y_true).long()\n",
        "            model.train()\n",
        "\n",
        "            # -------------------------------\n",
        "            # Train gates\n",
        "            # -------------------------------\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                exit_labels=exit_labels\n",
        "            )\n",
        "\n",
        "            loss = out[\"loss\"]\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.exit_mlps.parameters(),\n",
        "                hp[\"max_grad_norm\"]\n",
        "            )\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % hp[\"log_every\"] == 0:\n",
        "              print(\n",
        "                  f\"[Epoch {epoch+1}/{hp['num_epochs']}] \"\n",
        "                  f\"Step {step}/{len(train_loader)} | \"\n",
        "                  f\"Gate Loss: {loss.item():.4f}\"\n",
        "              )\n",
        "\n",
        "        avg_train_gate_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        val_gate_loss, val_gate_accuracy = evaluate_mlp_gates(\n",
        "            model, val_loader, device\n",
        "        )\n",
        "\n",
        "        epoch_metrics = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_gate_loss\": avg_train_gate_loss,\n",
        "            \"val_gate_loss\": val_gate_loss,\n",
        "            \"val_gate_accuracy\": val_gate_accuracy\n",
        "        }\n",
        "        epoch_history.append(epoch_metrics)\n",
        "\n",
        "        if val_gate_loss < best_val_gate_loss:\n",
        "            best_val_gate_loss = val_gate_loss\n",
        "            best_epoch_metrics = epoch_metrics\n",
        "\n",
        "            best_model_dir = os.path.join(exp_dir, \"best_model\")\n",
        "            os.makedirs(best_model_dir, exist_ok=True)\n",
        "            torch.save(\n",
        "                model.exit_mlps.state_dict(),\n",
        "                os.path.join(best_model_dir, \"mlp_gates.bin\")\n",
        "            )\n",
        "\n",
        "    return epoch_history, best_epoch_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "da9484f0-a496-4b6d-8983-a2b237754639",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da9484f0-a496-4b6d-8983-a2b237754639",
        "outputId": "91aac249-325b-40cf-e922-3efde9d6b068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "RUNNING MLP GATE EXPERIMENT: mlpH64_lr0.0001_wd0.0_ep2_drop0.0_batch_size16_gateW1.0\n",
            "Hyperparameters: {'num_labels': 2, 'dropout': 0.0, 'mlp_hidden_dim': 64, 'gate_loss_weight': 1.0, 'learning_rate': 0.0001, 'weight_decay': 0.0, 'num_epochs': 2, 'batch_size': 16, 'max_grad_norm': 1.0, 'log_every': 500}\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/2] Step 500/4210 | Gate Loss: 0.7335\n",
            "[Epoch 1/2] Step 1000/4210 | Gate Loss: 0.3618\n",
            "[Epoch 1/2] Step 1500/4210 | Gate Loss: 1.8584\n",
            "[Epoch 1/2] Step 2000/4210 | Gate Loss: 2.4675\n",
            "[Epoch 1/2] Step 2500/4210 | Gate Loss: 0.5358\n",
            "[Epoch 1/2] Step 3000/4210 | Gate Loss: 0.4075\n",
            "[Epoch 1/2] Step 3500/4210 | Gate Loss: 2.4481\n",
            "[Epoch 1/2] Step 4000/4210 | Gate Loss: 0.2797\n",
            "[Epoch 2/2] Step 500/4210 | Gate Loss: 1.0572\n",
            "[Epoch 2/2] Step 1000/4210 | Gate Loss: 0.6933\n",
            "[Epoch 2/2] Step 1500/4210 | Gate Loss: 0.1141\n",
            "[Epoch 2/2] Step 2000/4210 | Gate Loss: 2.5462\n",
            "[Epoch 2/2] Step 2500/4210 | Gate Loss: 1.4532\n",
            "[Epoch 2/2] Step 3000/4210 | Gate Loss: 2.2469\n",
            "[Epoch 2/2] Step 3500/4210 | Gate Loss: 0.5971\n",
            "[Epoch 2/2] Step 4000/4210 | Gate Loss: 0.3990\n",
            ">>> Finished mlpH64_lr0.0001_wd0.0_ep2_drop0.0_batch_size16_gateW1.0 | Best Val Gate Loss: 2.6939 | Best Val Gate Acc: 0.8949\n",
            "\n",
            "==============================\n",
            "RUNNING MLP GATE EXPERIMENT: mlpH64_lr0.0003_wd0.0_ep2_drop0.0_batch_size16_gateW1.0\n",
            "Hyperparameters: {'num_labels': 2, 'dropout': 0.0, 'mlp_hidden_dim': 64, 'gate_loss_weight': 1.0, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'num_epochs': 2, 'batch_size': 16, 'max_grad_norm': 1.0, 'log_every': 500}\n",
            "==============================\n",
            "[Epoch 1/2] Step 500/4210 | Gate Loss: 1.3237\n",
            "[Epoch 1/2] Step 1000/4210 | Gate Loss: 0.2409\n",
            "[Epoch 1/2] Step 1500/4210 | Gate Loss: 0.0967\n",
            "[Epoch 1/2] Step 2000/4210 | Gate Loss: 0.3319\n",
            "[Epoch 1/2] Step 2500/4210 | Gate Loss: 0.4974\n",
            "[Epoch 1/2] Step 3000/4210 | Gate Loss: 1.2268\n",
            "[Epoch 1/2] Step 3500/4210 | Gate Loss: 0.2975\n",
            "[Epoch 1/2] Step 4000/4210 | Gate Loss: 0.6520\n",
            "[Epoch 2/2] Step 500/4210 | Gate Loss: 1.9333\n",
            "[Epoch 2/2] Step 1000/4210 | Gate Loss: 0.6263\n",
            "[Epoch 2/2] Step 1500/4210 | Gate Loss: 3.4657\n",
            "[Epoch 2/2] Step 2000/4210 | Gate Loss: 0.3680\n",
            "[Epoch 2/2] Step 2500/4210 | Gate Loss: 0.3576\n",
            "[Epoch 2/2] Step 3000/4210 | Gate Loss: 1.6801\n",
            "[Epoch 2/2] Step 3500/4210 | Gate Loss: 0.7863\n",
            "[Epoch 2/2] Step 4000/4210 | Gate Loss: 0.0902\n",
            ">>> Finished mlpH64_lr0.0003_wd0.0_ep2_drop0.0_batch_size16_gateW1.0 | Best Val Gate Loss: 2.7049 | Best Val Gate Acc: 0.8954\n",
            "\n",
            "==============================\n",
            "RUNNING MLP GATE EXPERIMENT: mlpH64_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_gateW1.0\n",
            "Hyperparameters: {'num_labels': 2, 'dropout': 0.0, 'mlp_hidden_dim': 64, 'gate_loss_weight': 1.0, 'learning_rate': 3e-05, 'weight_decay': 0.0, 'num_epochs': 2, 'batch_size': 16, 'max_grad_norm': 1.0, 'log_every': 500}\n",
            "==============================\n",
            "[Epoch 1/2] Step 500/4210 | Gate Loss: 4.0270\n",
            "[Epoch 1/2] Step 1000/4210 | Gate Loss: 1.9520\n",
            "[Epoch 1/2] Step 1500/4210 | Gate Loss: 2.6237\n",
            "[Epoch 1/2] Step 2000/4210 | Gate Loss: 1.2562\n",
            "[Epoch 1/2] Step 2500/4210 | Gate Loss: 3.0866\n",
            "[Epoch 1/2] Step 3000/4210 | Gate Loss: 0.9689\n",
            "[Epoch 1/2] Step 3500/4210 | Gate Loss: 1.1059\n",
            "[Epoch 1/2] Step 4000/4210 | Gate Loss: 3.1947\n",
            "[Epoch 2/2] Step 500/4210 | Gate Loss: 1.2293\n",
            "[Epoch 2/2] Step 1000/4210 | Gate Loss: 0.7895\n",
            "[Epoch 2/2] Step 1500/4210 | Gate Loss: 2.0135\n",
            "[Epoch 2/2] Step 2000/4210 | Gate Loss: 0.8342\n",
            "[Epoch 2/2] Step 2500/4210 | Gate Loss: 0.6186\n",
            "[Epoch 2/2] Step 3000/4210 | Gate Loss: 0.7804\n",
            "[Epoch 2/2] Step 3500/4210 | Gate Loss: 0.9347\n",
            "[Epoch 2/2] Step 4000/4210 | Gate Loss: 0.2999\n",
            ">>> Finished mlpH64_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_gateW1.0 | Best Val Gate Loss: 2.4900 | Best Val Gate Acc: 0.8955\n",
            "\n",
            "==============================\n",
            "RUNNING MLP GATE EXPERIMENT: mlpH128_lr0.0001_wd0.0_ep2_drop0.0_batch_size16_gateW1.0\n",
            "Hyperparameters: {'num_labels': 2, 'dropout': 0.0, 'mlp_hidden_dim': 128, 'gate_loss_weight': 1.0, 'learning_rate': 0.0001, 'weight_decay': 0.0, 'num_epochs': 2, 'batch_size': 16, 'max_grad_norm': 1.0, 'log_every': 500}\n",
            "==============================\n",
            "[Epoch 1/2] Step 500/4210 | Gate Loss: 1.3638\n",
            "[Epoch 1/2] Step 1000/4210 | Gate Loss: 0.1142\n",
            "[Epoch 1/2] Step 1500/4210 | Gate Loss: 0.6504\n",
            "[Epoch 1/2] Step 2000/4210 | Gate Loss: 0.8018\n",
            "[Epoch 1/2] Step 2500/4210 | Gate Loss: 0.1891\n",
            "[Epoch 1/2] Step 3000/4210 | Gate Loss: 1.5122\n",
            "[Epoch 1/2] Step 3500/4210 | Gate Loss: 0.5112\n",
            "[Epoch 1/2] Step 4000/4210 | Gate Loss: 0.7353\n",
            "[Epoch 2/2] Step 500/4210 | Gate Loss: 0.3847\n",
            "[Epoch 2/2] Step 1000/4210 | Gate Loss: 0.7320\n",
            "[Epoch 2/2] Step 1500/4210 | Gate Loss: 1.3029\n",
            "[Epoch 2/2] Step 2000/4210 | Gate Loss: 0.2549\n",
            "[Epoch 2/2] Step 2500/4210 | Gate Loss: 2.0266\n",
            "[Epoch 2/2] Step 3000/4210 | Gate Loss: 1.1088\n",
            "[Epoch 2/2] Step 3500/4210 | Gate Loss: 0.6531\n",
            "[Epoch 2/2] Step 4000/4210 | Gate Loss: 2.7768\n",
            ">>> Finished mlpH128_lr0.0001_wd0.0_ep2_drop0.0_batch_size16_gateW1.0 | Best Val Gate Loss: 2.6045 | Best Val Gate Acc: 0.8968\n",
            "\n",
            "==============================\n",
            "RUNNING MLP GATE EXPERIMENT: mlpH128_lr0.0003_wd0.0_ep2_drop0.0_batch_size16_gateW1.0\n",
            "Hyperparameters: {'num_labels': 2, 'dropout': 0.0, 'mlp_hidden_dim': 128, 'gate_loss_weight': 1.0, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'num_epochs': 2, 'batch_size': 16, 'max_grad_norm': 1.0, 'log_every': 500}\n",
            "==============================\n",
            "[Epoch 1/2] Step 500/4210 | Gate Loss: 0.2956\n",
            "[Epoch 1/2] Step 1000/4210 | Gate Loss: 0.8530\n",
            "[Epoch 1/2] Step 1500/4210 | Gate Loss: 0.5348\n",
            "[Epoch 1/2] Step 2000/4210 | Gate Loss: 0.4571\n",
            "[Epoch 1/2] Step 2500/4210 | Gate Loss: 1.0401\n",
            "[Epoch 1/2] Step 3000/4210 | Gate Loss: 0.4625\n",
            "[Epoch 1/2] Step 3500/4210 | Gate Loss: 1.8396\n",
            "[Epoch 1/2] Step 4000/4210 | Gate Loss: 1.2248\n",
            "[Epoch 2/2] Step 500/4210 | Gate Loss: 0.5376\n",
            "[Epoch 2/2] Step 1000/4210 | Gate Loss: 0.6768\n",
            "[Epoch 2/2] Step 1500/4210 | Gate Loss: 1.0390\n",
            "[Epoch 2/2] Step 2000/4210 | Gate Loss: 0.3419\n",
            "[Epoch 2/2] Step 2500/4210 | Gate Loss: 0.9883\n",
            "[Epoch 2/2] Step 3000/4210 | Gate Loss: 1.7374\n",
            "[Epoch 2/2] Step 3500/4210 | Gate Loss: 0.9833\n",
            "[Epoch 2/2] Step 4000/4210 | Gate Loss: 0.2785\n",
            ">>> Finished mlpH128_lr0.0003_wd0.0_ep2_drop0.0_batch_size16_gateW1.0 | Best Val Gate Loss: 2.7251 | Best Val Gate Acc: 0.8958\n",
            "\n",
            "==============================\n",
            "RUNNING MLP GATE EXPERIMENT: mlpH128_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_gateW1.0\n",
            "Hyperparameters: {'num_labels': 2, 'dropout': 0.0, 'mlp_hidden_dim': 128, 'gate_loss_weight': 1.0, 'learning_rate': 3e-05, 'weight_decay': 0.0, 'num_epochs': 2, 'batch_size': 16, 'max_grad_norm': 1.0, 'log_every': 500}\n",
            "==============================\n",
            "[Epoch 1/2] Step 500/4210 | Gate Loss: 1.4943\n",
            "[Epoch 1/2] Step 1000/4210 | Gate Loss: 0.1410\n",
            "[Epoch 1/2] Step 1500/4210 | Gate Loss: 1.4742\n",
            "[Epoch 1/2] Step 2000/4210 | Gate Loss: 5.1183\n",
            "[Epoch 1/2] Step 2500/4210 | Gate Loss: 1.7386\n",
            "[Epoch 1/2] Step 3000/4210 | Gate Loss: 2.2314\n",
            "[Epoch 1/2] Step 3500/4210 | Gate Loss: 3.2930\n",
            "[Epoch 1/2] Step 4000/4210 | Gate Loss: 0.9435\n",
            "[Epoch 2/2] Step 500/4210 | Gate Loss: 0.5597\n",
            "[Epoch 2/2] Step 1000/4210 | Gate Loss: 1.9420\n",
            "[Epoch 2/2] Step 1500/4210 | Gate Loss: 2.4261\n",
            "[Epoch 2/2] Step 2000/4210 | Gate Loss: 0.2052\n",
            "[Epoch 2/2] Step 2500/4210 | Gate Loss: 4.0528\n",
            "[Epoch 2/2] Step 3000/4210 | Gate Loss: 0.8760\n",
            "[Epoch 2/2] Step 3500/4210 | Gate Loss: 1.0989\n",
            "[Epoch 2/2] Step 4000/4210 | Gate Loss: 1.0312\n",
            ">>> Finished mlpH128_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_gateW1.0 | Best Val Gate Loss: 2.4758 | Best Val Gate Acc: 0.8956\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# generate all hyperparameter combinations\n",
        "keys = list(hyperparameter_grid.keys())\n",
        "values = list(hyperparameter_grid.values())\n",
        "\n",
        "all_experiment_results = {}\n",
        "\n",
        "exit_layers = [3, 6, 9, 12, 15, 18, 21, 24]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "for combo in itertools.product(*values):\n",
        "\n",
        "    hp = dict(zip(keys, combo))\n",
        "    hp[\"num_labels\"] = 2\n",
        "\n",
        "    exp_name = make_experiment_name(hp)\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"RUNNING MLP GATE EXPERIMENT:\", exp_name)\n",
        "    print(\"Hyperparameters:\", hp)\n",
        "    print(\"==============================\")\n",
        "\n",
        "    # 1️⃣ Create model\n",
        "    model = GPT2EarlyExitClassifier(\n",
        "        model_name=\"gpt2-medium\",\n",
        "        exit_layers=exit_layers,\n",
        "        hyperparameters=hp\n",
        "    )\n",
        "\n",
        "    # 2️⃣ Load fine-tuned classifier checkpoint\n",
        "    state_dict = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    # 3️⃣ Move to GPU\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 4️⃣ Freeze backbone + exit heads (extra safety)\n",
        "    for p in model.model.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.exit_heads.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 5️⃣ Train MLP gates\n",
        "    train_history, best_epoch_metrics = train_mlp_gates(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=test_loader,\n",
        "        hp=hp,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # 6️⃣ Store results\n",
        "    all_experiment_results[exp_name] = {\n",
        "        \"hyperparameters\": hp,\n",
        "        \"train_history\": train_history,\n",
        "        \"best_epoch_metrics\": best_epoch_metrics,\n",
        "        \"best_val_gate_loss\": best_epoch_metrics[\"val_gate_loss\"],\n",
        "        \"best_val_gate_accuracy\": best_epoch_metrics[\"val_gate_accuracy\"]\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        f\">>> Finished {exp_name} | \"\n",
        "        f\"Best Val Gate Loss: {best_epoch_metrics['val_gate_loss']:.4f} | \"\n",
        "        f\"Best Val Gate Acc: {best_epoch_metrics['val_gate_accuracy']:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_experiment_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJEWNeubAe95",
        "outputId": "023c7ea0-a736-4036-b9e2-fe95a4e6e6f8"
      },
      "id": "KJEWNeubAe95",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mlpH64_lr0.0001_wd0.0_ep2_drop0.0_batch_size16_gateW1.0': {'hyperparameters': {'num_labels': 2,\n",
              "   'dropout': 0.0,\n",
              "   'mlp_hidden_dim': 64,\n",
              "   'gate_loss_weight': 1.0,\n",
              "   'learning_rate': 0.0001,\n",
              "   'weight_decay': 0.0,\n",
              "   'num_epochs': 2,\n",
              "   'batch_size': 16,\n",
              "   'max_grad_norm': 1.0,\n",
              "   'log_every': 500},\n",
              "  'train_history': [{'epoch': 1,\n",
              "    'train_gate_loss': 1.2374307141814054,\n",
              "    'val_gate_loss': 2.6938558592037722,\n",
              "    'val_gate_accuracy': 0.8949254587155964},\n",
              "   {'epoch': 2,\n",
              "    'train_gate_loss': 1.1649047161198074,\n",
              "    'val_gate_loss': 2.8530091236260806,\n",
              "    'val_gate_accuracy': 0.8946387614678899}],\n",
              "  'best_epoch_metrics': {'epoch': 1,\n",
              "   'train_gate_loss': 1.2374307141814054,\n",
              "   'val_gate_loss': 2.6938558592037722,\n",
              "   'val_gate_accuracy': 0.8949254587155964},\n",
              "  'best_val_gate_loss': 2.6938558592037722,\n",
              "  'best_val_gate_accuracy': 0.8949254587155964},\n",
              " 'mlpH64_lr0.0003_wd0.0_ep2_drop0.0_batch_size16_gateW1.0': {'hyperparameters': {'num_labels': 2,\n",
              "   'dropout': 0.0,\n",
              "   'mlp_hidden_dim': 64,\n",
              "   'gate_loss_weight': 1.0,\n",
              "   'learning_rate': 0.0003,\n",
              "   'weight_decay': 0.0,\n",
              "   'num_epochs': 2,\n",
              "   'batch_size': 16,\n",
              "   'max_grad_norm': 1.0,\n",
              "   'log_every': 500},\n",
              "  'train_history': [{'epoch': 1,\n",
              "    'train_gate_loss': 1.2202390390512425,\n",
              "    'val_gate_loss': 2.7048988048325886,\n",
              "    'val_gate_accuracy': 0.895355504587156},\n",
              "   {'epoch': 2,\n",
              "    'train_gate_loss': 1.1594336549185145,\n",
              "    'val_gate_loss': 2.715251669016751,\n",
              "    'val_gate_accuracy': 0.8959288990825688}],\n",
              "  'best_epoch_metrics': {'epoch': 1,\n",
              "   'train_gate_loss': 1.2202390390512425,\n",
              "   'val_gate_loss': 2.7048988048325886,\n",
              "   'val_gate_accuracy': 0.895355504587156},\n",
              "  'best_val_gate_loss': 2.7048988048325886,\n",
              "  'best_val_gate_accuracy': 0.895355504587156},\n",
              " 'mlpH64_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_gateW1.0': {'hyperparameters': {'num_labels': 2,\n",
              "   'dropout': 0.0,\n",
              "   'mlp_hidden_dim': 64,\n",
              "   'gate_loss_weight': 1.0,\n",
              "   'learning_rate': 3e-05,\n",
              "   'weight_decay': 0.0,\n",
              "   'num_epochs': 2,\n",
              "   'batch_size': 16,\n",
              "   'max_grad_norm': 1.0,\n",
              "   'log_every': 500},\n",
              "  'train_history': [{'epoch': 1,\n",
              "    'train_gate_loss': 1.2864068884676823,\n",
              "    'val_gate_loss': 2.489955643090335,\n",
              "    'val_gate_accuracy': 0.8954988532110092},\n",
              "   {'epoch': 2,\n",
              "    'train_gate_loss': 1.1691032441195781,\n",
              "    'val_gate_loss': 2.5310404073108326,\n",
              "    'val_gate_accuracy': 0.8959288990825688}],\n",
              "  'best_epoch_metrics': {'epoch': 1,\n",
              "   'train_gate_loss': 1.2864068884676823,\n",
              "   'val_gate_loss': 2.489955643090335,\n",
              "   'val_gate_accuracy': 0.8954988532110092},\n",
              "  'best_val_gate_loss': 2.489955643090335,\n",
              "  'best_val_gate_accuracy': 0.8954988532110092},\n",
              " 'mlpH128_lr0.0001_wd0.0_ep2_drop0.0_batch_size16_gateW1.0': {'hyperparameters': {'num_labels': 2,\n",
              "   'dropout': 0.0,\n",
              "   'mlp_hidden_dim': 128,\n",
              "   'gate_loss_weight': 1.0,\n",
              "   'learning_rate': 0.0001,\n",
              "   'weight_decay': 0.0,\n",
              "   'num_epochs': 2,\n",
              "   'batch_size': 16,\n",
              "   'max_grad_norm': 1.0,\n",
              "   'log_every': 500},\n",
              "  'train_history': [{'epoch': 1,\n",
              "    'train_gate_loss': 1.2277051041238984,\n",
              "    'val_gate_loss': 2.8923601220954547,\n",
              "    'val_gate_accuracy': 0.8947821100917431},\n",
              "   {'epoch': 2,\n",
              "    'train_gate_loss': 1.1652658753919984,\n",
              "    'val_gate_loss': 2.604477745700966,\n",
              "    'val_gate_accuracy': 0.8967889908256881}],\n",
              "  'best_epoch_metrics': {'epoch': 2,\n",
              "   'train_gate_loss': 1.1652658753919984,\n",
              "   'val_gate_loss': 2.604477745700966,\n",
              "   'val_gate_accuracy': 0.8967889908256881},\n",
              "  'best_val_gate_loss': 2.604477745700966,\n",
              "  'best_val_gate_accuracy': 0.8967889908256881},\n",
              " 'mlpH128_lr0.0003_wd0.0_ep2_drop0.0_batch_size16_gateW1.0': {'hyperparameters': {'num_labels': 2,\n",
              "   'dropout': 0.0,\n",
              "   'mlp_hidden_dim': 128,\n",
              "   'gate_loss_weight': 1.0,\n",
              "   'learning_rate': 0.0003,\n",
              "   'weight_decay': 0.0,\n",
              "   'num_epochs': 2,\n",
              "   'batch_size': 16,\n",
              "   'max_grad_norm': 1.0,\n",
              "   'log_every': 500},\n",
              "  'train_history': [{'epoch': 1,\n",
              "    'train_gate_loss': 1.2358378313048224,\n",
              "    'val_gate_loss': 2.7419120199301026,\n",
              "    'val_gate_accuracy': 0.8952121559633027},\n",
              "   {'epoch': 2,\n",
              "    'train_gate_loss': 1.1637128503895817,\n",
              "    'val_gate_loss': 2.725079741870815,\n",
              "    'val_gate_accuracy': 0.8957855504587156}],\n",
              "  'best_epoch_metrics': {'epoch': 2,\n",
              "   'train_gate_loss': 1.1637128503895817,\n",
              "   'val_gate_loss': 2.725079741870815,\n",
              "   'val_gate_accuracy': 0.8957855504587156},\n",
              "  'best_val_gate_loss': 2.725079741870815,\n",
              "  'best_val_gate_accuracy': 0.8957855504587156},\n",
              " 'mlpH128_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_gateW1.0': {'hyperparameters': {'num_labels': 2,\n",
              "   'dropout': 0.0,\n",
              "   'mlp_hidden_dim': 128,\n",
              "   'gate_loss_weight': 1.0,\n",
              "   'learning_rate': 3e-05,\n",
              "   'weight_decay': 0.0,\n",
              "   'num_epochs': 2,\n",
              "   'batch_size': 16,\n",
              "   'max_grad_norm': 1.0,\n",
              "   'log_every': 500},\n",
              "  'train_history': [{'epoch': 1,\n",
              "    'train_gate_loss': 1.2621279524201239,\n",
              "    'val_gate_loss': 2.475754916126078,\n",
              "    'val_gate_accuracy': 0.8956422018348624},\n",
              "   {'epoch': 2,\n",
              "    'train_gate_loss': 1.1740158423091198,\n",
              "    'val_gate_loss': 2.726499204743992,\n",
              "    'val_gate_accuracy': 0.8952121559633027}],\n",
              "  'best_epoch_metrics': {'epoch': 1,\n",
              "   'train_gate_loss': 1.2621279524201239,\n",
              "   'val_gate_loss': 2.475754916126078,\n",
              "   'val_gate_accuracy': 0.8956422018348624},\n",
              "  'best_val_gate_loss': 2.475754916126078,\n",
              "  'best_val_gate_accuracy': 0.8956422018348624}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_experiment_results(all_results):\n",
        "    \"\"\"\n",
        "    Flattens MLP-gate experiment results into a CSV-friendly list of dicts.\n",
        "    Each row corresponds to ONE hyperparameter configuration\n",
        "    and its BEST saved gate model.\n",
        "    \"\"\"\n",
        "    flat_data = []\n",
        "\n",
        "    for exp_name, data in all_results.items():\n",
        "        row = {\n",
        "            \"experiment_name\": exp_name\n",
        "        }\n",
        "\n",
        "        # -----------------------------\n",
        "        # Best model metrics (MLP gate)\n",
        "        # -----------------------------\n",
        "        if \"best_train_metrics\" in data and data[\"best_train_metrics\"] is not None:\n",
        "            row[\"best_epoch\"] = data[\"best_train_metrics\"].get(\"epoch\")\n",
        "            row[\"best_train_gate_loss\"] = data[\"best_train_metrics\"].get(\"gate_loss\")\n",
        "        else:\n",
        "            row[\"best_epoch\"] = None\n",
        "            row[\"best_train_gate_loss\"] = None\n",
        "\n",
        "        # Validation metrics (if available)\n",
        "        row[\"val_gate_loss\"] = data.get(\"val_gate_loss\")\n",
        "        row[\"val_gate_accuracy\"] = data.get(\"val_gate_accuracy\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Hyperparameters\n",
        "        # -----------------------------\n",
        "        for hp_key, hp_value in data[\"hyperparameters\"].items():\n",
        "            if isinstance(hp_value, list):\n",
        "                row[f\"hp_{hp_key}\"] = str(hp_value)\n",
        "            else:\n",
        "                row[f\"hp_{hp_key}\"] = hp_value\n",
        "\n",
        "        flat_data.append(row)\n",
        "\n",
        "    return flat_data\n",
        "\n",
        "print(\"\\n\\n--- Saving All Experiment Results to CSV ---\")\n",
        "\n",
        "flat_results = flatten_experiment_results(all_experiment_results)\n",
        "\n",
        "results_df = pd.DataFrame(flat_results)\n",
        "\n",
        "csv_filename = \"all_mlp_gate_results.csv\"\n",
        "csv_path = os.path.join(PERSISTENT_BASE_DIR, csv_filename)\n",
        "\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"✅ All experiment results saved to: {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMISTaPH_N-M",
        "outputId": "3ed2d1a9-de82-4bb5-e3b7-7f96a9d72e5e"
      },
      "id": "OMISTaPH_N-M",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Saving All Experiment Results to CSV ---\n",
            "✅ All experiment results saved to: /content/drive/MyDrive/Efficient_AI_Project/EarlyExit_Experiments/komal/gpt2-medium-MLP/all_mlp_gate_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "PbRNzjUmmV80",
        "outputId": "b3cfa5a7-5f54-4f06-ebdf-5e7f625096ba"
      },
      "id": "PbRNzjUmmV80",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     experiment_name best_epoch  \\\n",
              "0  mlpH64_lr0.0001_wd0.0_ep2_drop0.0_batch_size16...       None   \n",
              "1  mlpH64_lr0.0003_wd0.0_ep2_drop0.0_batch_size16...       None   \n",
              "2  mlpH64_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_...       None   \n",
              "3  mlpH128_lr0.0001_wd0.0_ep2_drop0.0_batch_size1...       None   \n",
              "4  mlpH128_lr0.0003_wd0.0_ep2_drop0.0_batch_size1...       None   \n",
              "5  mlpH128_lr3e-05_wd0.0_ep2_drop0.0_batch_size16...       None   \n",
              "\n",
              "  best_train_gate_loss val_gate_loss val_gate_accuracy  hp_num_labels  \\\n",
              "0                 None          None              None              2   \n",
              "1                 None          None              None              2   \n",
              "2                 None          None              None              2   \n",
              "3                 None          None              None              2   \n",
              "4                 None          None              None              2   \n",
              "5                 None          None              None              2   \n",
              "\n",
              "   hp_dropout  hp_mlp_hidden_dim  hp_gate_loss_weight  hp_learning_rate  \\\n",
              "0         0.0                 64                  1.0           0.00010   \n",
              "1         0.0                 64                  1.0           0.00030   \n",
              "2         0.0                 64                  1.0           0.00003   \n",
              "3         0.0                128                  1.0           0.00010   \n",
              "4         0.0                128                  1.0           0.00030   \n",
              "5         0.0                128                  1.0           0.00003   \n",
              "\n",
              "   hp_weight_decay  hp_num_epochs  hp_batch_size  hp_max_grad_norm  \\\n",
              "0              0.0              2             16               1.0   \n",
              "1              0.0              2             16               1.0   \n",
              "2              0.0              2             16               1.0   \n",
              "3              0.0              2             16               1.0   \n",
              "4              0.0              2             16               1.0   \n",
              "5              0.0              2             16               1.0   \n",
              "\n",
              "   hp_log_every  \n",
              "0           500  \n",
              "1           500  \n",
              "2           500  \n",
              "3           500  \n",
              "4           500  \n",
              "5           500  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-220b8f2e-72f2-4bff-a10e-fa4cdd4a9bc2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment_name</th>\n",
              "      <th>best_epoch</th>\n",
              "      <th>best_train_gate_loss</th>\n",
              "      <th>val_gate_loss</th>\n",
              "      <th>val_gate_accuracy</th>\n",
              "      <th>hp_num_labels</th>\n",
              "      <th>hp_dropout</th>\n",
              "      <th>hp_mlp_hidden_dim</th>\n",
              "      <th>hp_gate_loss_weight</th>\n",
              "      <th>hp_learning_rate</th>\n",
              "      <th>hp_weight_decay</th>\n",
              "      <th>hp_num_epochs</th>\n",
              "      <th>hp_batch_size</th>\n",
              "      <th>hp_max_grad_norm</th>\n",
              "      <th>hp_log_every</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mlpH64_lr0.0001_wd0.0_ep2_drop0.0_batch_size16...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mlpH64_lr0.0003_wd0.0_ep2_drop0.0_batch_size16...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mlpH64_lr3e-05_wd0.0_ep2_drop0.0_batch_size16_...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mlpH128_lr0.0001_wd0.0_ep2_drop0.0_batch_size1...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>128</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mlpH128_lr0.0003_wd0.0_ep2_drop0.0_batch_size1...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>128</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mlpH128_lr3e-05_wd0.0_ep2_drop0.0_batch_size16...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>128</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-220b8f2e-72f2-4bff-a10e-fa4cdd4a9bc2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-220b8f2e-72f2-4bff-a10e-fa4cdd4a9bc2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-220b8f2e-72f2-4bff-a10e-fa4cdd4a9bc2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKQfMnKImWll"
      },
      "id": "HKQfMnKImWll",
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}