{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZI01r0zl-O4O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI01r0zl-O4O",
        "outputId": "6314eae0-cb94-42ab-a774-dca828df8ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'adaptive-inference-llm' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "!git clone https://github.com/komalniraula/adaptive-inference-llm\n",
        "\n",
        "repo_name = 'adaptive-inference-llm' # Must match the folder created by git clone\n",
        "project_path = os.path.join('/content', repo_name)\n",
        "\n",
        "# Append the project root directory to the system path\n",
        "\n",
        "sys.path.append(project_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e26c312-8a54-4dbc-bd5d-2d935f8d9d9b",
      "metadata": {
        "id": "6e26c312-8a54-4dbc-bd5d-2d935f8d9d9b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "449b3275-a509-45d5-a415-3faa06510a06",
      "metadata": {
        "id": "449b3275-a509-45d5-a415-3faa06510a06"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, ModernBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7893e799-33bf-4ccf-a19b-8157127350ea",
      "metadata": {
        "id": "7893e799-33bf-4ccf-a19b-8157127350ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e93123-4625-4c39-f499-6194ab5c412b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets once...\n",
            "\n",
            "Loading sst2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the sst2 dataset: 872\n",
            "\n",
            "All datasets loaded.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "import time\n",
        "\n",
        "from evaluation.dataset_loaders.sst2 import load_sst2\n",
        "\n",
        "dataset_loaders = [\n",
        "    (\"sst2\", load_sst2, \"classification\"),\n",
        "    ]\n",
        "\n",
        "cached_datasets = {}\n",
        "print(\"Loading datasets once...\\n\")\n",
        "\n",
        "for name, loader, task in dataset_loaders:\n",
        "    print(f\"Loading {name}...\")\n",
        "    cached_datasets[name] = {\n",
        "        \"data\": loader(task='test', fraction=1),\n",
        "        \"task\": task\n",
        "    }\n",
        "\n",
        "for data in cached_datasets:\n",
        "    print(f\"Length of the {data} dataset: {len(cached_datasets[data]['data'])}\")\n",
        "\n",
        "print(\"\\nAll datasets loaded.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbafd28-6379-4d55-963c-79550ba0a743",
      "metadata": {
        "id": "5fbafd28-6379-4d55-963c-79550ba0a743"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "class RoBERTaLargeBaselineClassifier:\n",
        "    def __init__(self, model_name=\"siebert/sentiment-roberta-large-english\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Load tokenizer + fine-tuned RoBERTa-large sentiment classifier\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # RoBERTa-large has 24 encoder layers\n",
        "        self.num_layers = self.model.config.num_hidden_layers  # = 24\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def classify(self, text, dataset_name=None):\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "        )\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Forward pass through full model\n",
        "        outputs = self.model(**inputs)\n",
        "        logits = outputs.logits  # shape [1, 2]\n",
        "\n",
        "        # Softmax probabilities\n",
        "        probs = F.softmax(logits, dim=-1)[0]\n",
        "        conf, pred = torch.max(probs, dim=0)\n",
        "\n",
        "        # Return: predicted label, #layers used, confidence\n",
        "        return pred.item(), self.num_layers, conf.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f208a237-3bf9-4644-9323-1e578692f02b",
      "metadata": {
        "id": "f208a237-3bf9-4644-9323-1e578692f02b"
      },
      "outputs": [],
      "source": [
        "class DistilBERTBaselineClassifier:\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Load tokenizer + fine-tuned DistilBERT sentiment classifier\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # DistilBERT has 6 encoder layers\n",
        "        self.num_layers = self.model.config.num_hidden_layers  # = 6\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def classify(self, text, dataset_name=None):\n",
        "        # Tokenize text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=False\n",
        "        )\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(**inputs)\n",
        "        logits = outputs.logits  # shape [1, 2]\n",
        "\n",
        "        # Compute probabilities\n",
        "        probs = F.softmax(logits, dim=-1)[0]\n",
        "        conf, pred = torch.max(probs, dim=0)\n",
        "\n",
        "        # Baseline â†’ always uses full 6 layers\n",
        "        return pred.item(), self.num_layers, conf.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c88862-88be-4b05-a563-6dab52f81bd1",
      "metadata": {
        "id": "27c88862-88be-4b05-a563-6dab52f81bd1"
      },
      "outputs": [],
      "source": [
        "# Extract (text, label) from any format\n",
        "def extract_text_label(sample):\n",
        "    if isinstance(sample, dict):\n",
        "        if \"text\" in sample:\n",
        "            return sample[\"text\"], sample[\"label\"]\n",
        "        elif \"sentence\" in sample:\n",
        "            return sample[\"sentence\"], sample[\"label\"]\n",
        "        elif \"input_text\" in sample:\n",
        "            return sample[\"input_text\"], sample[\"label\"]\n",
        "        else:\n",
        "            raise ValueError(\"Unknown dict format:\", sample)\n",
        "\n",
        "    if isinstance(sample, (tuple, list)):\n",
        "        return sample[0], sample[1]\n",
        "\n",
        "    raise ValueError(\"Unknown sample format:\", sample)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_dataset(model, dataset, dataset_name):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    layers_used = []\n",
        "    total_tokens = 0\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for sample in dataset:\n",
        "        text, label = extract_text_label(sample)\n",
        "\n",
        "        pred, layer, conf = model.classify(text, dataset_name)\n",
        "\n",
        "        correct += (pred == label)\n",
        "        total += 1\n",
        "        layers_used.append(layer)\n",
        "\n",
        "        total_tokens += len(model.tokenizer(text)[\"input_ids\"])\n",
        "\n",
        "    end = time.time()\n",
        "    latency = (end - start) / total\n",
        "\n",
        "    return {\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"score\": correct / total,\n",
        "        \"avg_latency_sec\": latency,\n",
        "        \"tokens_per_sec\": total_tokens / (end - start),\n",
        "        \"avg_layers_used\": float(np.mean(layers_used)),\n",
        "        \"num_samples\": total\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f55be5-411f-4909-8554-a5456a36cd3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55f55be5-411f-4909-8554-a5456a36cd3f",
        "outputId": "9aa69044-0421-4fde-9a5e-31425d7707cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running BASELINE DISTILBERT finetuned on SST2\n",
            "DistilBERT sst2 finetuned model layers: 6\n",
            "\n",
            "Testing BASELINE on sst2...\n",
            "sst2 {'metric': 'accuracy', 'score': 0.9105504587155964, 'avg_latency_sec': 0.004897614684673624, 'tokens_per_sec': 5138.009509901858, 'avg_layers_used': 6.0, 'num_samples': 872}\n"
          ]
        }
      ],
      "source": [
        "results_table = []\n",
        "\n",
        "print(\"Running BASELINE DISTILBERT finetuned on SST2\")\n",
        "\n",
        "baseline_model = DistilBERTBaselineClassifier()\n",
        "\n",
        "# Print number of layers (for logging/reporting)\n",
        "print(\"DistilBERT sst2 finetuned model layers:\", baseline_model.num_layers)\n",
        "\n",
        "for name, meta in cached_datasets.items():\n",
        "    dataset = meta[\"data\"]\n",
        "    print(f\"\\nTesting BASELINE on {name}...\")\n",
        "\n",
        "    result = evaluate_dataset(baseline_model, dataset, name)\n",
        "    print(name, result)\n",
        "\n",
        "    results_table.append({\n",
        "        \"dataset\": name,\n",
        "        \"threshold\": None,\n",
        "        \"mode\": \"baseline\",\n",
        "        \"model\": \"Distil-BERT sst finetuned\",\n",
        "        \"metric\": result[\"metric\"],\n",
        "        \"score\": float(result[\"score\"]),\n",
        "        \"avg_latency_sec\": float(result[\"avg_latency_sec\"]),\n",
        "        \"tokens_per_sec\": float(result[\"tokens_per_sec\"]),\n",
        "        \"avg_layers_used\": float(result[\"avg_layers_used\"]),\n",
        "        \"num_samples\": int(result[\"num_samples\"]),\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52e27a7-9484-4f60-b5c2-9e81c54362da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52e27a7-9484-4f60-b5c2-9e81c54362da",
        "outputId": "9d2edf9f-11cf-45bc-e47d-a6ca4188c93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running BASELINE DISTILBERT uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT uncased model layers: 6\n",
            "\n",
            "Testing BASELINE on sst2...\n",
            "sst2 {'metric': 'accuracy', 'score': 0.9105504587155964, 'avg_latency_sec': 0.004632979904839752, 'tokens_per_sec': 5431.491468245093, 'avg_layers_used': 6.0, 'num_samples': 872}\n"
          ]
        }
      ],
      "source": [
        "print(\"Running BASELINE DISTILBERT uncased\")\n",
        "\n",
        "baseline_model_distil_uncased = DistilBERTBaselineClassifier(model_name=\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "# Print number of layers (for logging/reporting)\n",
        "print(\"DistilBERT uncased model layers:\", baseline_model_distil_uncased.num_layers)\n",
        "\n",
        "for name, meta in cached_datasets.items():\n",
        "    dataset = meta[\"data\"]\n",
        "    print(f\"\\nTesting BASELINE on {name}...\")\n",
        "\n",
        "    result = evaluate_dataset(baseline_model, dataset, name)\n",
        "    print(name, result)\n",
        "\n",
        "    results_table.append({\n",
        "        \"dataset\": name,\n",
        "        \"threshold\": None,\n",
        "        \"mode\": \"baseline\",\n",
        "        \"model\": \"Distil-BERT uncased\",\n",
        "        \"metric\": result[\"metric\"],\n",
        "        \"score\": float(result[\"score\"]),\n",
        "        \"avg_latency_sec\": float(result[\"avg_latency_sec\"]),\n",
        "        \"tokens_per_sec\": float(result[\"tokens_per_sec\"]),\n",
        "        \"avg_layers_used\": float(result[\"avg_layers_used\"]),\n",
        "        \"num_samples\": int(result[\"num_samples\"]),\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7b2510-7eb2-4156-b4a6-6fe34b2a940d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e7b2510-7eb2-4156-b4a6-6fe34b2a940d",
        "outputId": "baff2995-2504-4182-861c-9dd63ec2c2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running BASELINE ROBERTA\n",
            "RoBERTa Large model layers: 24\n",
            "\n",
            "Testing BASELINE on sst2...\n",
            "sst2 {'metric': 'accuracy', 'score': 0.9243119266055045, 'avg_latency_sec': 0.015610218321511505, 'tokens_per_sec': 1666.1633922239846, 'avg_layers_used': 24.0, 'num_samples': 872}\n"
          ]
        }
      ],
      "source": [
        "print(\"Running BASELINE ROBERTA\")\n",
        "\n",
        "baseline_model_roberta = RoBERTaLargeBaselineClassifier()\n",
        "\n",
        "print(\"RoBERTa Large model layers:\", baseline_model_roberta.num_layers)\n",
        "\n",
        "for name, meta in cached_datasets.items():\n",
        "    dataset = meta[\"data\"]\n",
        "    print(f\"\\nTesting BASELINE on {name}...\")\n",
        "\n",
        "    result = evaluate_dataset(baseline_model_roberta, dataset, name)\n",
        "    print(name, result)\n",
        "\n",
        "    results_table.append({\n",
        "        \"dataset\": name,\n",
        "        \"threshold\": None,\n",
        "        \"mode\": \"baseline\",\n",
        "        \"model\": \"ROBERTA-Large\",\n",
        "        \"metric\": result[\"metric\"],\n",
        "        \"score\": float(result[\"score\"]),\n",
        "        \"avg_latency_sec\": float(result[\"avg_latency_sec\"]),\n",
        "        \"tokens_per_sec\": float(result[\"tokens_per_sec\"]),\n",
        "        \"avg_layers_used\": float(result[\"avg_layers_used\"]),\n",
        "        \"num_samples\": int(result[\"num_samples\"]),\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7728d549-eb48-4e26-ae24-ab7ecdd40a66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "7728d549-eb48-4e26-ae24-ab7ecdd40a66",
        "outputId": "5fb0e331-259c-423c-caf4-60b2b615c7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-126550988.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.sort_values(\"score\", ascending=False))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  dataset threshold      mode                      model    metric     score  \\\n",
              "0    sst2      None  baseline              ROBERTA-Large  accuracy  0.924312   \n",
              "1    sst2      None  baseline  Distil-BERT sst finetuned  accuracy  0.910550   \n",
              "2    sst2      None  baseline        Distil-BERT uncased  accuracy  0.910550   \n",
              "\n",
              "   avg_latency_sec  tokens_per_sec  avg_layers_used  num_samples  \n",
              "0         0.015610     1666.163392             24.0          872  \n",
              "1         0.004898     5138.009510              6.0          872  \n",
              "2         0.004633     5431.491468              6.0          872  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d7fee8da-e104-4c7d-b997-3522650b14d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>threshold</th>\n",
              "      <th>mode</th>\n",
              "      <th>model</th>\n",
              "      <th>metric</th>\n",
              "      <th>score</th>\n",
              "      <th>avg_latency_sec</th>\n",
              "      <th>tokens_per_sec</th>\n",
              "      <th>avg_layers_used</th>\n",
              "      <th>num_samples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sst2</td>\n",
              "      <td>None</td>\n",
              "      <td>baseline</td>\n",
              "      <td>ROBERTA-Large</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0.924312</td>\n",
              "      <td>0.015610</td>\n",
              "      <td>1666.163392</td>\n",
              "      <td>24.0</td>\n",
              "      <td>872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sst2</td>\n",
              "      <td>None</td>\n",
              "      <td>baseline</td>\n",
              "      <td>Distil-BERT sst finetuned</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0.910550</td>\n",
              "      <td>0.004898</td>\n",
              "      <td>5138.009510</td>\n",
              "      <td>6.0</td>\n",
              "      <td>872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sst2</td>\n",
              "      <td>None</td>\n",
              "      <td>baseline</td>\n",
              "      <td>Distil-BERT uncased</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0.910550</td>\n",
              "      <td>0.004633</td>\n",
              "      <td>5431.491468</td>\n",
              "      <td>6.0</td>\n",
              "      <td>872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7fee8da-e104-4c7d-b997-3522650b14d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d7fee8da-e104-4c7d-b997-3522650b14d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d7fee8da-e104-4c7d-b997-3522650b14d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df=pd.DataFrame(results_table)\n",
        "df_sorted = (\n",
        "    df.groupby(\"dataset\", group_keys=True)\n",
        "      .apply(lambda g: g.sort_values(\"score\", ascending=False))\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "df_sorted"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}