{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f35dfa-e61e-456a-a8af-c0aa7959ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples: 75879\n",
      "Total test samples: 1938\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import itertools\n",
    "# -----------------------------\n",
    "# Load datasets\n",
    "# -----------------------------\n",
    "def load_sst2():\n",
    "    ds = load_dataset(\"glue\", \"sst2\")\n",
    "    return ds[\"train\"], ds[\"validation\"]\n",
    "\n",
    "def load_rotten():\n",
    "    ds = load_dataset(\"rotten_tomatoes\")\n",
    "    return ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "sst2_train, sst2_test = load_sst2()\n",
    "rt_train, rt_test = load_rotten()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Convert datasets to (text,label)\n",
    "# -----------------------------\n",
    "def convert_sst2(sample):\n",
    "    return sample[\"sentence\"], int(sample[\"label\"])\n",
    "\n",
    "def convert_rotten(sample):\n",
    "    return sample[\"text\"], int(sample[\"label\"])\n",
    "\n",
    "train_pairs = [convert_sst2(x) for x in sst2_train] + \\\n",
    "              [convert_rotten(x) for x in rt_train]\n",
    "\n",
    "test_pairs  = [convert_sst2(x) for x in sst2_test] + \\\n",
    "              [convert_rotten(x) for x in rt_test]\n",
    "\n",
    "# Optional shuffle\n",
    "random.shuffle(train_pairs)\n",
    "random.shuffle(test_pairs)\n",
    "\n",
    "print(f\"Total train samples: {len(train_pairs)}\")\n",
    "print(f\"Total test samples: {len(test_pairs)}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset class\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dynamic padding collate_fn\n",
    "# -----------------------------\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"labels\"] for b in batch])\n",
    "\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    padded = torch.full((len(batch), max_len), pad_id)\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        padded[i, :len(ids)] = ids\n",
    "\n",
    "    # attention mask: float\n",
    "    attention_mask = (padded != tokenizer.eos_token_id).float()\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685c08e0-67bf-426e-8bad-b7d2d7ac8242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader batches: 4743\n",
      "Test loader batches: 122\n",
      "torch.Size([16, 44])\n",
      "torch.Size([16, 44])\n",
      "tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Create datasets\n",
    "# -----------------------------\n",
    "train_ds = SentimentDataset(train_pairs, tokenizer)\n",
    "test_ds = SentimentDataset(test_pairs, tokenizer)\n",
    "\n",
    "# -----------------------------\n",
    "# Create dataloaders\n",
    "# -----------------------------\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"Train loader batches:\", len(train_loader))\n",
    "print(\"Test loader batches:\", len(test_loader))\n",
    "\n",
    "# Optional: Inspect first batch\n",
    "example = next(iter(train_loader))\n",
    "print(example[\"input_ids\"].shape)\n",
    "print(example[\"attention_mask\"].shape)\n",
    "print(example[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a3861d-1488-47b8-8eff-d36eb44f02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "class GPT2EarlyExitClassifier(nn.Module):\n",
    "    def __init__(self, model_name, exit_layers, hyperparameters):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load GPT-2 as causal LM (we will use only hidden states)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            output_hidden_states=True,   # <-- IMPORTANT\n",
    "            return_dict=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.exit_layers = sorted(exit_layers)\n",
    "        self.hp = hyperparameters\n",
    "        self.num_labels = self.hp.get(\"num_labels\", 2)\n",
    "        dropout_rate = self.hp.get(\"dropout\", 0.0)\n",
    "\n",
    "        # Loss weights Î»_e for each exit\n",
    "        self.exit_loss_weights = self.hp.get(\n",
    "            \"exit_loss_weights\",\n",
    "            [1.0] * len(self.exit_layers)\n",
    "        )\n",
    "\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        # Create classification heads for each exit layer\n",
    "        self.exit_heads = nn.ModuleDict()\n",
    "        for layer in self.exit_layers:\n",
    "            self.exit_heads[str(layer)] = nn.Sequential(\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_size, self.num_labels)\n",
    "            )\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        - Calls GPT-2 normally (causal mask automatically handled)\n",
    "        - Retrieves hidden_states for each layer\n",
    "        - Applies classifier at each exit layer\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # hidden_states is a tuple:\n",
    "        # [0] = embedding output\n",
    "        # [1] = layer 1 output\n",
    "        # ...\n",
    "        # [12] = last layer output  (if GPT2 base)\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        logits_dict = {}\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # For each early exit\n",
    "        for i, layer in enumerate(self.exit_layers):\n",
    "\n",
    "            # hidden_states[layer] has shape [batch, seq_len, hidden_dim]\n",
    "            cls_vec = hidden_states[layer][:, -1, :]   # last token rep\n",
    "\n",
    "            logits = self.exit_heads[str(layer)](cls_vec)\n",
    "            logits_dict[layer] = logits\n",
    "\n",
    "            # Add weighted loss\n",
    "            if labels is not None:\n",
    "                weight = self.exit_loss_weights[i]\n",
    "                total_loss += weight * self.ce(logits, labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss if labels is not None else None,\n",
    "            \"logits\": logits_dict\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b706b3-305c-404f-ba74-2ef0782f9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {\n",
    "    \"num_labels\": [2],                 # fixed\n",
    "    \"dropout\": [0.0],\n",
    "    \"exit_loss_weights\": [\n",
    "        [1, 1, 1, 1, 1, 1, 1]],\n",
    "\n",
    "    # training hyperparams\n",
    "    \"learning_rate\": [1e-5],\n",
    "    \"weight_decay\": [0.01],\n",
    "    \"num_epochs\": [3],\n",
    "    \"max_grad_norm\": [1.0],\n",
    "    \"batch_size\": [16],\n",
    "\n",
    "    # logging\n",
    "    \"log_every\": [1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a759cc13-cba0-4574-b7ed-91fea9c2c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experiment_name(hp):\n",
    "    return (\n",
    "        f\"lr{hp['learning_rate']}_\"\n",
    "        f\"wd{hp['weight_decay']}_\"\n",
    "        f\"ep{hp['num_epochs']}_\"\n",
    "        f\"drop{hp['dropout']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ef9a9a-9043-42f1-8a68-8973abbb19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, hp, device):\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hp[\"learning_rate\"],\n",
    "        weight_decay=hp[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    for epoch in range(hp[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(train_loader, start=1):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            out = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "\n",
    "            loss = out[\"loss\"]\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), hp[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % hp[\"log_every\"] == 0:\n",
    "                print(f\"Epoch {epoch+1} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"\\n>>> Epoch {epoch+1} completed | Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Run evaluation\n",
    "        evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9484f0-a496-4b6d-8983-a2b237754639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "RUNNING EXPERIMENT: lr1e-05_wd0.01_ep3_drop0.0\n",
      "Hyperparameters: {'num_labels': 2, 'dropout': 0.0, 'exit_loss_weights': [1, 1, 1, 1, 1, 1, 1], 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 3, 'max_grad_norm': 1.0, 'batch_size': 16, 'log_every': 1000}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "exit_layers = [3, 6, 9, 12, 15, 18, 21]\n",
    "\n",
    "# generate all hyperparameter combinations\n",
    "keys = list(hyperparameter_grid.keys())\n",
    "values = list(hyperparameter_grid.values())\n",
    "\n",
    "for combo in itertools.product(*values):\n",
    "    \n",
    "    hp = dict(zip(keys, combo))\n",
    "    hp[\"num_labels\"] = 2  # fixed\n",
    "    \n",
    "    exp_name = make_experiment_name(hp)\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"RUNNING EXPERIMENT:\", exp_name)\n",
    "    print(\"Hyperparameters:\", hp)\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # 1. Create fresh model for this configuration\n",
    "    model = GPT2EarlyExitClassifier(\n",
    "        model_name=\"gpt2-medium\",\n",
    "        exit_layers=exit_layers,\n",
    "        hyperparameters=hp\n",
    "    ).to(device)\n",
    "\n",
    "    # 2. Train\n",
    "    train(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        hp=hp,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 3. Save checkpoint\n",
    "    save_path = f\"./checkpoints/{exp_name}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    print(f\">>> Saved model to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880b305-3df8-4e5e-bc8f-a308c4661a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
