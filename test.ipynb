{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2730f4-0ca3-4436-8977-c8993f13024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer               \n",
    "\n",
    "from evaluation.evaluator import EarlyExitEvaluator\n",
    "from strategies.confidence_exit import ConfidenceExit\n",
    "from models.gpt2_wrapper import GPT2WithEarlyExit\n",
    "from evaluation.dataset_loaders.sst2 import load_sst2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5fedfd-4de9-43d3-9a9d-d66ccadce706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|███████████████████████████████| 50/50 [00:01<00:00, 26.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'accuracy', 'score': np.float64(0.74), 'avg_latency_sec': np.float64(0.03759671688079834), 'tokens_per_sec': 26.598067144281075, 'avg_layers_used': np.float64(11.88), 'num_samples': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# simple test one sentiment data\n",
    "strategy = ConfidenceExit(threshold=1, allowed_layers=[3,6,9])\n",
    "model = GPT2WithEarlyExit(\"gpt2\", strategy, tokenizer)\n",
    "\n",
    "dataset = load_sst2(number=50, task=\"train\")   # Number of data to use = 100, without kv \n",
    "\n",
    "evaluator = EarlyExitEvaluator(tokenizer)\n",
    "\n",
    "result = evaluator.evaluate(\n",
    "    model=model,\n",
    "    strategy=strategy,\n",
    "    dataset=dataset,\n",
    "    task_type=\"classification\",\n",
    "    dataset_name=\"sst2\",\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b9d7d-91f9-41f7-9c2f-ec62a0b7b19c",
   "metadata": {},
   "source": [
    "### KV-Cache Behavior in Early-Exit GPT-2 Wrapper\n",
    "\n",
    "Early-exit GPT-2 wrapper supports two execution modes depending on the task.\n",
    "KV-cache is handled differently for classification vs generation tasks.\n",
    "\n",
    "#### Classification Tasks (SST-2, AGNews) — No KV-Cache Used\n",
    "\n",
    "#### Generation Tasks (Summarization, Translation, QA)\n",
    "\n",
    "For generation, two modes depending on use_kv parameter. \n",
    "if use_kv = False (Full Recompute, slow mode)\n",
    "- Every new token recomputes all layers\n",
    "- Early exit only skips layers inside one forward pass\n",
    "- KV-cache is not stored\n",
    "- Useful for reproducing naive early-exit results\n",
    "\n",
    "if use_kv = True (KV Vached Early Exit, fast mode)\n",
    "- step 1: encode the prompt once and produce hidden states for the prompt, KV pair for every layer\n",
    "- step 2: decode tokens with early exit:\n",
    "    - for each token run layers sequenctiall..\n",
    "    - At each layer, compute confidence\n",
    "        - If early exit triggers at layer L:\n",
    "        - Layers 0..L compute normally and update KV\n",
    "        - Layers L+1..final are skipped\n",
    "        - Their KV is copied forward unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4aa213a-7eaf-42b7-88b9-635c937ca815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sst2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 23.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst2 {'metric': 'accuracy', 'score': np.float64(1.0), 'avg_latency_sec': np.float64(0.04217815399169922), 'tokens_per_sec': 23.70895606756054, 'avg_layers_used': np.float64(4.0), 'num_samples': 1}\n",
      "Testing agnews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 15/15 [00:00<00:00, 44.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agnews {'metric': 'accuracy', 'score': np.float64(0.26666666666666666), 'avg_latency_sec': np.float64(0.02246535619099935), 'tokens_per_sec': 44.51298218902248, 'avg_layers_used': np.float64(5.2), 'num_samples': 15}\n",
      "Testing cnn_dm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Evaluating: 100%|███████████████████████████████| 22/22 [00:14<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_dm {'metric': 'rougeL', 'score': np.float64(0.05079575095102862), 'avg_latency_sec': np.float64(0.6509716402400624), 'tokens_per_sec': 1.5361652308405087, 'avg_layers_used': np.float64(4.051136363636363), 'num_samples': 22}\n",
      "Testing wmt14_enfr...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8931dda2a9284160a6b0024ed0c84a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8016ca362d5648d5a46075fe59bde2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████| 6/6 [00:03<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wmt14_enfr {'metric': 'bleu', 'score': np.float64(0.0), 'avg_latency_sec': np.float64(0.5234866937001547), 'tokens_per_sec': 1.910268230376043, 'avg_layers_used': np.float64(4.015625), 'num_samples': 6}\n",
      "Testing squad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 21/21 [00:09<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad {'metric': 'token_f1', 'score': np.float64(0.004896882945663434), 'avg_latency_sec': np.float64(0.4498912152789888), 'tokens_per_sec': 2.222759560619282, 'avg_layers_used': np.float64(4.017857142857143), 'num_samples': 21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### testing with KV similar to CALM paper\n",
    "from evaluation.dataset_loaders.sst2 import load_sst2\n",
    "from evaluation.dataset_loaders.agnews import load_agnews\n",
    "from evaluation.dataset_loaders.cnn_dm import load_cnndm\n",
    "from evaluation.dataset_loaders.squad import load_squad\n",
    "from evaluation.dataset_loaders.wmt_en_fr import load_wmt_enfr\n",
    "\n",
    "strategy = ConfidenceExit(threshold=0.7, allowed_layers=[3,6,9])\n",
    "model = GPT2WithEarlyExit(\"gpt2\", strategy, tokenizer, use_kv=\"True\")\n",
    "\n",
    "evaluator = EarlyExitEvaluator(tokenizer)\n",
    "\n",
    "datasets = [\n",
    "    (\"sst2\", load_sst2, \"classification\"),\n",
    "    (\"agnews\", load_agnews, \"classification\"),\n",
    "    (\"cnn_dm\", load_cnndm, \"summarization\"),\n",
    "    (\"wmt14_enfr\", load_wmt_enfr, \"translation\"),\n",
    "    (\"squad\", load_squad, \"qa\"),\n",
    "]\n",
    "\n",
    "for name, loader, task in datasets:\n",
    "    print(f\"Testing {name}...\")\n",
    "\n",
    "    dataset = loader(fraction=0.002)\n",
    "\n",
    "    # ---------- IMPORTANT: pass dataset_name for classification ----------\n",
    "    if task == \"classification\":\n",
    "        result = evaluator.evaluate(\n",
    "            model=model,\n",
    "            strategy=strategy,\n",
    "            dataset=dataset,\n",
    "            task_type=task,\n",
    "            dataset_name=name,      # e.g. \"sst2\" or \"agnews\"\n",
    "        )\n",
    "    else:\n",
    "        result = evaluator.evaluate(\n",
    "            model=model,\n",
    "            strategy=strategy,\n",
    "            dataset=dataset,\n",
    "            task_type=task,\n",
    "        )\n",
    "\n",
    "    print(name, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358ea6ce-89cf-4288-ab0f-10232d52e2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sst2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 24.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst2 {'metric': 'accuracy', 'score': np.float64(1.0), 'avg_latency_sec': np.float64(0.040064096450805664), 'tokens_per_sec': 24.96000380859433, 'avg_layers_used': np.float64(4.0), 'num_samples': 1}\n",
      "Testing agnews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 15/15 [00:00<00:00, 41.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agnews {'metric': 'accuracy', 'score': np.float64(0.26666666666666666), 'avg_latency_sec': np.float64(0.024156729380289715), 'tokens_per_sec': 41.39633243629138, 'avg_layers_used': np.float64(5.2), 'num_samples': 15}\n",
      "Testing cnn_dm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 22/22 [00:14<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_dm {'metric': 'rougeL', 'score': np.float64(0.05079575095102862), 'avg_latency_sec': np.float64(0.6464643586765636), 'tokens_per_sec': 1.546875688626039, 'avg_layers_used': np.float64(4.051136363636363), 'num_samples': 22}\n",
      "Testing wmt14_enfr...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c355c7562e4c2c87e2637e4fef1dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084f9ce167f04520a520109d28989434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████| 6/6 [00:03<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wmt14_enfr {'metric': 'bleu', 'score': np.float64(0.0), 'avg_latency_sec': np.float64(0.5050203402837118), 'tokens_per_sec': 1.980118265015261, 'avg_layers_used': np.float64(4.015625), 'num_samples': 6}\n",
      "Testing squad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 21/21 [00:08<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad {'metric': 'token_f1', 'score': np.float64(0.004896882945663434), 'avg_latency_sec': np.float64(0.42584298905872164), 'tokens_per_sec': 2.3482833478376346, 'avg_layers_used': np.float64(4.017857142857143), 'num_samples': 21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### testing without kv\n",
    "strategy = ConfidenceExit(threshold=0.7, allowed_layers=[3,6,9])\n",
    "model = GPT2WithEarlyExit(\"gpt2\", strategy, tokenizer, use_kv=\"False\")\n",
    "\n",
    "evaluator = EarlyExitEvaluator(tokenizer)\n",
    "\n",
    "datasets = [\n",
    "    (\"sst2\", load_sst2, \"classification\"),\n",
    "    (\"agnews\", load_agnews, \"classification\"),\n",
    "    (\"cnn_dm\", load_cnndm, \"summarization\"),\n",
    "    (\"wmt14_enfr\", load_wmt_enfr, \"translation\"),\n",
    "    (\"squad\", load_squad, \"qa\"),\n",
    "]\n",
    "\n",
    "for name, loader, task in datasets:\n",
    "    print(f\"Testing {name}...\")\n",
    "\n",
    "    dataset = loader(fraction=0.002)\n",
    "\n",
    "    # pass dataset_name for classification\n",
    "    if task == \"classification\":\n",
    "        result = evaluator.evaluate(\n",
    "            model=model,\n",
    "            strategy=strategy,\n",
    "            dataset=dataset,\n",
    "            task_type=task,\n",
    "            dataset_name=name,      # e.g. \"sst2\" or \"agnews\"\n",
    "        )\n",
    "    else:\n",
    "        result = evaluator.evaluate(\n",
    "            model=model,\n",
    "            strategy=strategy,\n",
    "            dataset=dataset,\n",
    "            task_type=task,\n",
    "        )\n",
    "\n",
    "    print(name, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b21e3b-0642-4c6a-b557-dacd60caf463",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Strategy 2 - Confidence threshold should be (meet) in Continous layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1019086-7a15-4161-be7c-1c01ea7ff572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategies.continous_confidence_exit import ContinuousConfidenceExit\n",
    "\n",
    "strategy = ContinuousConfidenceExit(\n",
    "    threshold=0.75,\n",
    "    required_consecutive=2,\n",
    "    allowed_layers=[3, 6, 9, 11]\n",
    ")\n",
    "\n",
    "model = GPT2WithEarlyExit(\"gpt2\", strategy, tokenizer)\n",
    "evaluator = EarlyExitEvaluator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1257d5-01f6-4cfb-9706-41cbdae3ed5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
